{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variational Autoencoders(VAEs) from Scratch\n",
    "We will considers few points starting our implementation of the VAEs\n",
    "1. Predict $log(variance)$ instead of variance. It's done for stability during training process like $\\mu$ for mean and $\\log(\\sigma^2)$ for variance which are positive. we can achieve same by using RELU but it will not have well defined gradient transiants. Variance have smaller value $\\sigma \\in [0, 1]$ which can have a wider range value after the log transform $log(\\sigma) \\in [-\\inf, +\\inf]$ i.e $log : [0,1] \\to [-\\inf, +\\inf]$.\n",
    "2. Reparameterization Tricks : We needed to sample data from $q(z|x)$ butif we directly sample from it's introduces stochastic elements but backpropagation relies on the deterministic elements. Thus we needed to do randomness for sampling but also preserve the differentiability of operations involves. The random sampling $z \\approx N(0, 1)$ and then scale and translate to $z*\\sigma + \\mu$. Now backpropagation continuously flows through deterministic  nodes $\\mu$ and $\\sigma$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import modules\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Define hyperparameters\n",
    "batch_size = 100\n",
    "latent_dim = 2\n",
    "epochs = 100\n",
    "\n",
    "# Load MNIST dataset\n",
    "transform = transforms.ToTensor()\n",
    "train_dataset = datasets.MNIST('~/.pytorch/MNIST_data/', download=True, train=True, transform=transform)\n",
    "test_dataset = datasets.MNIST('~/.pytorch/MNIST_data/', download=True, train=False, transform=transform)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define VAE model\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, input_dim, latent_dim):\n",
    "        super(VAE, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, latent_dim * 2)\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, input_dim),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def encode(self, x):\n",
    "        z_mean, z_log_var = self.encoder(x).chunk(2, dim=1)\n",
    "        return z_mean, z_log_var\n",
    "\n",
    "    def reparameterize(self, z_mean, z_log_var):\n",
    "        std = torch.exp(0.5 * z_log_var)\n",
    "        eps = torch.randn_like(std)\n",
    "        return z_mean + eps * std\n",
    "\n",
    "    def decode(self, z):\n",
    "        return self.decoder(z)\n",
    "\n",
    "    def forward(self, x):\n",
    "        z_mean, z_log_var = self.encode(x)\n",
    "        z = self.reparameterize(z_mean, z_log_var)\n",
    "        x_recon = self.decode(z)\n",
    "        return x_recon, z_mean, z_log_var\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize VAE and optimizer\n",
    "vae = VAE(input_dim=784, latent_dim=latent_dim)\n",
    "optimizer = optim.Adam(vae.parameters(), lr=0.001)\n",
    "\n",
    "# Train VAE\n",
    "for epoch in range(epochs):\n",
    "    for x, _ in train_loader:\n",
    "        x = x.view(-1, 784)\n",
    "        x_recon, z_mean, z_log_var = vae(x)\n",
    "        reconstruction_loss = nn.MSELoss()(x_recon, x)\n",
    "        kl_divergence_loss = 0.5 * torch.sum(torch.exp(z_log_var) + z_mean ** 2 - 1 - z_log_var)\n",
    "        loss = reconstruction_loss + kl_divergence_loss\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f'Epoch {epoch+1}, Loss: {loss.item()}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test VAE\n",
    "with torch.no_grad():\n",
    "    for x, _ in test_loader:\n",
    "        x = x.view(-1, 784)\n",
    "        x_recon, _, _ = vae(x)\n",
    "        print(torch.sum((x_recon - x) ** 2))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
