{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1309710b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ajeet/miniconda3/envs/distributedsystems/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/tmp/ipykernel_2764575/3817544335.py:22: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html\n",
      "  from pkg_resources import packaging\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from transformers import BertTokenizer\n",
    "import os\n",
    "import pandas as pd\n",
    "import os\n",
    "import skimage\n",
    "import IPython.display\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "from collections import OrderedDict\n",
    "import torch\n",
    "from collections import OrderedDict\n",
    "from torch.optim import Adam\n",
    "import numpy as np\n",
    "import torch\n",
    "from pkg_resources import packaging\n",
    "import torch.nn.functional as F\n",
    "from pkg_resources import packaging\n",
    "import clip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "87952ac2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'AI504_Practice14'...\n",
      "remote: Enumerating objects: 7, done.\u001b[K\n",
      "remote: Counting objects: 100% (7/7), done.\u001b[K\n",
      "remote: Compressing objects: 100% (5/5), done.\u001b[K\n",
      "remote: Total 7 (delta 0), reused 0 (delta 0), pack-reused 0 (from 0)\u001b[K\n",
      "Receiving objects: 100% (7/7), 1.30 MiB | 4.70 MiB/s, done.\n",
      "mv: cannot move 'AI504_Practice14/simple_tokenizer.py' to '/content/': Not a directory\n",
      "mv: cannot move 'AI504_Practice14/bpe_simple_vocab_16e6.txt.gz' to '/content/': Not a directory\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/ttumyche/AI504_Practice14.git\n",
    "!mv AI504_Practice14/simple_tokenizer.py /content/\n",
    "!mv AI504_Practice14/bpe_simple_vocab_16e6.txt.gz /content/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d9e9f268",
   "metadata": {},
   "outputs": [],
   "source": [
    "from AI504_Practice14.simple_tokenizer import SimpleTokenizer\n",
    "def tokenize_text(tokenizer, text, max_length):\n",
    "    tokens = tokenizer.encode(text)\n",
    "    if len(tokens) == 0:\n",
    "        print(f\"Empty tokens for text: {text}\")\n",
    "        return torch.tensor([], dtype=torch.long)\n",
    "\n",
    "    tokens = tokens[:max_length]\n",
    "    if len(tokens) < max_length:\n",
    "        tokens.extend([0] * (max_length - len(tokens)))\n",
    "\n",
    "    return torch.tensor(tokens, dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "05712e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -q https://github.com/jbrownlee/Datasets/releases/download/Flickr8k/Flickr8k_Dataset.zip\n",
    "!wget -q https://github.com/jbrownlee/Datasets/releases/download/Flickr8k/Flickr8k_text.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ca6efbef",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "08b68ecf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: cannot remove 'Flicker8k_Dataset': Is a directory\n"
     ]
    }
   ],
   "source": [
    "!rm -f Flicker8k_Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "71db0f67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  datasets/Flickr8k_Dataset.zip\n",
      "replace Flicker8k_Dataset/1000268201_693b08cb0e.jpg? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
     ]
    }
   ],
   "source": [
    "!unzip datasets/Flickr8k_Dataset.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5a6477ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  datasets/Flickr8k_text.zip\n",
      "  inflating: CrowdFlowerAnnotations.txt  \n",
      "  inflating: ExpertAnnotations.txt   \n",
      "  inflating: Flickr8k.lemma.token.txt  \n",
      "   creating: __MACOSX/\n",
      "  inflating: __MACOSX/._Flickr8k.lemma.token.txt  \n",
      "  inflating: Flickr8k.token.txt      \n",
      "  inflating: Flickr_8k.devImages.txt  \n",
      "  inflating: Flickr_8k.testImages.txt  \n",
      "  inflating: Flickr_8k.trainImages.txt  \n",
      "  inflating: readme.txt              \n"
     ]
    }
   ],
   "source": [
    "!unzip datasets/Flickr8k_text.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e52381b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "import torch\n",
    "\n",
    "from PIL import Image\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "class Flickr8kDataset(Dataset):\n",
    "    def __init__(self, image_dir, captions_file, split_file, transform=None, tokenizer=None, max_length=77):\n",
    "        # Load and prepare data\n",
    "        self.image_dir = image_dir\n",
    "        self.df = pd.read_csv(captions_file, sep='\\t', names=['image', 'caption'])\n",
    "        self.split_file = pd.read_csv(split_file, names=['image'])\n",
    "\n",
    "        # Filter dataset based on split\n",
    "        self.df['image'] = self.df['image'].apply(lambda x: x.split('#')[0])\n",
    "        self.df = self.df[self.df['image'].isin(self.split_file['image'])]\n",
    "\n",
    "        # Remove empty captions\n",
    "        self.df = self.df[self.df['caption'].str.strip() != '']\n",
    "\n",
    "        self.transform = transform\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = os.path.join(self.image_dir, self.df.iloc[idx, 0])\n",
    "        image = Image.open(img_name).convert(\"RGB\")\n",
    "        caption = self.df.iloc[idx, 1]\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        if self.tokenizer:\n",
    "            caption = tokenize_text(self.tokenizer, caption, self.max_length)\n",
    "            if len(caption) == 0:\n",
    "                print(f\"Empty caption at index {idx}: {self.df.iloc[idx, 1]}\")\n",
    "        else:\n",
    "            caption = torch.tensor([], dtype=torch.long)\n",
    "\n",
    "        return image, caption\n",
    "# Image transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "96ac464b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import clip\n",
    "tokenizer = clip.tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a809f47f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Flickr8kDataset(\n",
    "    image_dir='Flicker8k_Dataset',\n",
    "    captions_file='Flickr8k.token.txt',\n",
    "    split_file='Flickr_8k.trainImages.txt',\n",
    "    transform=transform,\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=77\n",
    ")\n",
    "\n",
    "dev_dataset = Flickr8kDataset(\n",
    "    image_dir='Flicker8k_Dataset',\n",
    "    captions_file='Flickr8k.token.txt',\n",
    "    split_file='Flickr_8k.devImages.txt',\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=77\n",
    ")\n",
    "\n",
    "test_dataset = Flickr8kDataset(\n",
    "    image_dir='Flicker8k_Dataset',\n",
    "    captions_file='Flickr8k.token.txt',\n",
    "    split_file='Flickr_8k.testImages.txt',\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=77\n",
    ")\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "dev_dataloader = DataLoader(dev_dataset, batch_size=8, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=8, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1cee0904",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.CLIP import CLIP\n",
    "from src.Training import Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1c5af41c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ImageEncoder' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[25]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m     10\u001b[39m trans_heads = \u001b[32m8\u001b[39m\n\u001b[32m     11\u001b[39m trans_layers = \u001b[32m6\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m model = \u001b[43mCLIP\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m    \u001b[49m\u001b[43membed_dim\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m512\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m    \u001b[49m\u001b[43mimg_res\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m224\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvision_layers\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m12\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvision_width\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m768\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvision_patch\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m16\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvision_heads\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m8\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcontext_length\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m77\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvocab_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m49408\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrans_width\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m512\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrans_heads\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m8\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrans_layers\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m6\u001b[39;49m\n\u001b[32m     25\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     27\u001b[39m optimizer = optim.Adam(model.parameters(), lr=\u001b[32m1e-4\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/GPUTorch/CLIP/src/CLIP.py:12\u001b[39m, in \u001b[36mCLIP.__init__\u001b[39m\u001b[34m(self, embed_dim, img_res, vision_layers, vision_width, vision_heads, vision_patch, context_length, vocab_size, trans_width, trans_heads, trans_layers)\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, embed_dim, img_res, vision_layers, vision_width, vision_heads, vision_patch, context_length, vocab_size, trans_width, trans_heads, trans_layers):\n\u001b[32m     10\u001b[39m     \u001b[38;5;28msuper\u001b[39m().\u001b[34m__init__\u001b[39m()\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m     \u001b[38;5;28mself\u001b[39m.image_encoder = \u001b[43mImageEncoder\u001b[49m.ImageEncoder(\n\u001b[32m     13\u001b[39m         input_res=img_res,\n\u001b[32m     14\u001b[39m         patch_size=vision_patch,\n\u001b[32m     15\u001b[39m         width=vision_width,\n\u001b[32m     16\u001b[39m         layers=vision_layers,\n\u001b[32m     17\u001b[39m         heads=vision_heads,\n\u001b[32m     18\u001b[39m         ouptut_dim=embed_dim\n\u001b[32m     19\u001b[39m     )\n\u001b[32m     21\u001b[39m     \u001b[38;5;28mself\u001b[39m.text_encoder = TextEncoder.TextEncoder(\n\u001b[32m     22\u001b[39m         vocab_size=vocab_size,\n\u001b[32m     23\u001b[39m         trans_width=trans_width,\n\u001b[32m   (...)\u001b[39m\u001b[32m     27\u001b[39m         embed_dim=embed_dim\n\u001b[32m     28\u001b[39m     )\n\u001b[32m     30\u001b[39m     \u001b[38;5;28mself\u001b[39m.logit_scale = nn.Parameter(torch.ones([]) * np.log(\u001b[32m1\u001b[39m / \u001b[32m0.07\u001b[39m))\n",
      "\u001b[31mNameError\u001b[39m: name 'ImageEncoder' is not defined"
     ]
    }
   ],
   "source": [
    "img_res = 224\n",
    "vision_patch = 16\n",
    "vision_width = 768\n",
    "vision_layers = 4\n",
    "vision_heads = 12\n",
    "embed_dim = 128\n",
    "context_length = 77\n",
    "vocab_size = 49408\n",
    "trans_width = 512\n",
    "trans_heads = 8\n",
    "trans_layers = 6\n",
    "\n",
    "model = CLIP(\n",
    "    embed_dim=512,\n",
    "    img_res=224,\n",
    "    vision_layers=12,\n",
    "    vision_width=768,\n",
    "    vision_patch=16,\n",
    "    vision_heads=8,\n",
    "    context_length=77,\n",
    "    vocab_size=49408,\n",
    "    trans_width=512,\n",
    "    trans_heads=8,\n",
    "    trans_layers=6\n",
    ")\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21a8df7c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "distributedsystems",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
