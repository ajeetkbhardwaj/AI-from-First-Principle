{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab-1 : Attension\n",
    "\n",
    "Before, I address what is self-attension ? first we review how RNN maintaining a hidden state allows to incorporate its representation of previous words/vectors it has processed with the current one itâ€™s processing. Thus, Self attension is the method used by transformer for understanding the relevence of other words from vocabulary to the currect processing word in a given sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task-0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# python from scratch library\n",
    "import math\n",
    "import numpy as np \n",
    "\n",
    "# \n",
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task-1\n",
    "\n",
    "Attension - In a given sentence, It keep track every single word have some kind of affinity towards other word or not\n",
    "\n",
    "$$ \\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the query, key and value matrices\n",
    "def softmax(x):\n",
    "    return ( np.exp(x).T / np.sum(np.exp(x), axis=-1)).T\n",
    "\n",
    "def scaled_dot_product_attention( q, k, v, mask=None): # mask is none by default and is in encoder setting which \n",
    "                                                       # can be changed to decoder by passing mask\n",
    "    d_k=q.shape[-1]\n",
    "    scaled= np.matmul(q, k.T)/math.sqrt(d_k)\n",
    "    if mask is not None:\n",
    "        scaled= scaled+mask\n",
    "    print(\"scaled\", scaled)\n",
    "    attention= softmax(scaled)\n",
    "    out=np.matmul(attention, v)\n",
    "    return out, attention # new values , attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scaled [[0.92437508 0.76109342 0.57632375 0.62003569]\n",
      " [0.90323288 0.74314335 0.46591913 0.53600242]\n",
      " [0.93105229 0.80788533 0.59086872 0.60714951]\n",
      " [0.67256557 0.58922815 0.40639233 0.41051461]]\n",
      "Output:\n",
      " [[0.58980873 0.65315452 0.58977675 0.75164092 0.60384918 0.42990405\n",
      "  0.47024889 0.76868091]\n",
      " [0.58973423 0.65413257 0.59438378 0.74689309 0.60508062 0.43462382\n",
      "  0.47582019 0.76445329]\n",
      " [0.59206776 0.6592164  0.59325714 0.74994701 0.60851148 0.42665397\n",
      "  0.47231488 0.76395251]\n",
      " [0.5953361  0.66124875 0.59208973 0.75184169 0.6111631  0.41866359\n",
      "  0.46786183 0.76384738]]\n",
      "Attention:\n",
      " [[0.30367215 0.25792453 0.21441156 0.22399176]\n",
      " [0.31343185 0.26706509 0.2024045  0.21709857]\n",
      " [0.30128865 0.26637413 0.21440895 0.21792827]\n",
      " [0.28936702 0.26622942 0.22174379 0.22265977]]\n",
      "scaled [[0.92437508       -inf       -inf       -inf]\n",
      " [0.90323288 0.74314335       -inf       -inf]\n",
      " [0.93105229 0.80788533 0.59086872       -inf]\n",
      " [0.67256557 0.58922815 0.40639233 0.41051461]]\n",
      "Output with mask:\n",
      " [[0.31471367 0.48442819 0.55264977 0.74357197 0.35676674 0.95091744\n",
      "  0.68359855 0.89641877]\n",
      " [0.56971384 0.71954319 0.6978605  0.6576692  0.64068021 0.54892821\n",
      "  0.61849438 0.66553524]\n",
      " [0.55796366 0.78811287 0.64194667 0.74641224 0.6533264  0.47041898\n",
      "  0.56330402 0.70460616]\n",
      " [0.5953361  0.66124875 0.59208973 0.75184169 0.6111631  0.41866359\n",
      "  0.46786183 0.76384738]]\n",
      "Attention with mask:\n",
      " [[1.         0.         0.         0.        ]\n",
      " [0.53993713 0.46006287 0.         0.        ]\n",
      " [0.38524426 0.34060063 0.2741551  0.        ]\n",
      " [0.28936702 0.26622942 0.22174379 0.22265977]]\n"
     ]
    }
   ],
   "source": [
    "# length of input sequence\n",
    "L=4\n",
    "# size of thes vectors\n",
    "d_k = 8\n",
    "d_v = 8\n",
    "# initialize the q k v matrices\n",
    "q = np.random.rand(L, d_k)\n",
    "k = np.random.rand(L, d_k)\n",
    "v = np.random.rand(L, d_v)\n",
    "\n",
    "# compute the attention\n",
    "out, attention = scaled_dot_product_attention(q, k, v)\n",
    "# print the output and attention\n",
    "print(\"Output:\\n\", out)\n",
    "print(\"Attention:\\n\", attention)\n",
    "\n",
    "# create a mask for the decoder - prediction of the future tokens\n",
    "mask= np.tril(np.ones((L,L)))\n",
    "mask[mask==0]=-np.inf\n",
    "mask[mask==1]=0\n",
    "\n",
    "# compute the attention with mask\n",
    "out_masked, attention_masked = scaled_dot_product_attention(q, k, v, mask)\n",
    "# print the output and attention with mask\n",
    "print(\"Output with mask:\\n\", out_masked)\n",
    "print(\"Attention with mask:\\n\", attention_masked)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task-2 : Multi-Head Attension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product(q, k, v, mask=None):\n",
    "    d_k = q.size()[-1]\n",
    "    scaled = torch.matmul(q, k.transpose(-1, -2)) / math.sqrt(d_k)\n",
    "    if mask is not None:\n",
    "        scaled += mask\n",
    "    attention = F.softmax(scaled, dim=-1)\n",
    "    values = torch.matmul(attention, v)\n",
    "    return values, attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Can't call numpy() on Tensor that requires grad. Use tensor.detach().numpy() instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 21\u001b[39m\n\u001b[32m     18\u001b[39m \u001b[38;5;66;03m# now we will divide the chuck of each head into q, k, v\u001b[39;00m\n\u001b[32m     19\u001b[39m q, k, v=qkv.chunk(\u001b[32m3\u001b[39m, dim=-\u001b[32m1\u001b[39m) \u001b[38;5;66;03m# divide last dimension by 3 i.e 192/3 = 64\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m values, attention = scaled_dot_product(q, k, v, mask=mask)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 5\u001b[39m, in \u001b[36mscaled_dot_product\u001b[39m\u001b[34m(q, k, v, mask)\u001b[39m\n\u001b[32m      3\u001b[39m scaled = torch.matmul(q, k.transpose(-\u001b[32m1\u001b[39m, -\u001b[32m2\u001b[39m)) / math.sqrt(d_k)\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m     scaled += mask\n\u001b[32m      6\u001b[39m attention = F.softmax(scaled, dim=-\u001b[32m1\u001b[39m)\n\u001b[32m      7\u001b[39m values = torch.matmul(attention, v)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/ai-agents/lib/python3.13/site-packages/torch/_tensor.py:1225\u001b[39m, in \u001b[36mTensor.__array__\u001b[39m\u001b[34m(self, dtype)\u001b[39m\n\u001b[32m   1223\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(Tensor.__array__, (\u001b[38;5;28mself\u001b[39m,), \u001b[38;5;28mself\u001b[39m, dtype=dtype)\n\u001b[32m   1224\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1225\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.numpy()\n\u001b[32m   1226\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1227\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.numpy().astype(dtype, copy=\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[31mRuntimeError\u001b[39m: Can't call numpy() on Tensor that requires grad. Use tensor.detach().numpy() instead."
     ]
    }
   ],
   "source": [
    "sequence_length = 4 # My name is Ankit\n",
    "batch_size = 1 # number of sentences in batch\n",
    "input_dim = 512 # vector dimension of each input\n",
    "d_model = 512 # vector dimension of each output  of each attention unit       \n",
    "x = torch.rand((batch_size, sequence_length, input_dim)) # input tensor\n",
    "\n",
    "qkv_layer = nn.Linear(input_dim, 3*d_model) # here we are concatenating q, k, v in one matrix which will be split later\n",
    "qkv = qkv_layer(x)\n",
    "\n",
    "num_heads= 8 # now we have 8 attention head\n",
    "head_dim= d_model//num_heads # each head will have dim of 512 / 8 => 64\n",
    "qkv = qkv.reshape(batch_size, sequence_length, num_heads, 3*head_dim)\n",
    "\n",
    "# let's change sequencing in qkv just to make things easy in future and perform parallel operations of last 2 dimensions\n",
    "qkv=qkv.permute(0, 2, 1, 3) # [batch_size, num_heads, sequence_length, 3*head_dim]\n",
    "qkv.shape\n",
    "\n",
    "# now we will divide the chuck of each head into q, k, v\n",
    "q, k, v=qkv.chunk(3, dim=-1) # divide last dimension by 3 i.e 192/3 = 64\n",
    "\n",
    "values, attention = scaled_dot_product(q, k, v, mask=mask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, d_model, num_heads):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_model // num_heads\n",
    "        self.qkv_layer = nn.Linear(input_dim , 3 * d_model)\n",
    "        self.linear_layer = nn.Linear(d_model, d_model)\n",
    "    \n",
    "    def forward(self, x, mask=None): # forward pass\n",
    "        \n",
    "        batch_size, sequence_length, input_dim = x.size()\n",
    "        print(f\"x.size(): {x.size()}\\n\")\n",
    "        print(\"-\"*50)\n",
    "\n",
    "        qkv = self.qkv_layer(x)\n",
    "        print(f\"qkv.size(): {qkv.size()}\\n\")\n",
    "        print(\"-\"*50)\n",
    "\n",
    "        qkv = qkv.reshape(batch_size, sequence_length, self.num_heads, 3 * self.head_dim)\n",
    "        print(f\"qkv.size(): {qkv.size()}\")\n",
    "        qkv = qkv.permute(0, 2, 1, 3)\n",
    "        print(f\"qkv.size(): {qkv.size()}\\n\")\n",
    "        print(\"-\"*50)\n",
    "\n",
    "        q, k, v = qkv.chunk(3, dim=-1)\n",
    "        print(f\"q size: {q.size()}     k size: {k.size()}     v size: {v.size()}\\n\")\n",
    "        print(\"-\"*50)\n",
    "\n",
    "        values, attention = scaled_dot_product(q, k, v, mask)\n",
    "        print(f\"values.size(): {values.size()}          attention.size:{ attention.size()} \")\n",
    "        print(\"-\"*50)\n",
    "        values = values.reshape(batch_size, sequence_length, self.num_heads * self.head_dim)\n",
    "        print(f\"values.size(): {values.size()}\\n\")\n",
    "        print(\"-\"*50)\n",
    "\n",
    "        out = self.linear_layer(values)\n",
    "        print(f\"out.size(): {out.size()}\\n\")\n",
    "        print(\"-\"*50)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.size(): torch.Size([30, 5, 512])\n",
      "\n",
      "--------------------------------------------------\n",
      "qkv.size(): torch.Size([30, 5, 1536])\n",
      "\n",
      "--------------------------------------------------\n",
      "qkv.size(): torch.Size([30, 5, 8, 192])\n",
      "qkv.size(): torch.Size([30, 8, 5, 192])\n",
      "\n",
      "--------------------------------------------------\n",
      "q size: torch.Size([30, 8, 5, 64])     k size: torch.Size([30, 8, 5, 64])     v size: torch.Size([30, 8, 5, 64])\n",
      "\n",
      "--------------------------------------------------\n",
      "values.size(): torch.Size([30, 8, 5, 64])          attention.size:torch.Size([30, 8, 5, 5]) \n",
      "--------------------------------------------------\n",
      "values.size(): torch.Size([30, 5, 512])\n",
      "\n",
      "--------------------------------------------------\n",
      "out.size(): torch.Size([30, 5, 512])\n",
      "\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "input_dim = 512\n",
    "d_model = 512\n",
    "num_heads = 8\n",
    "\n",
    "batch_size = 30\n",
    "sequence_length = 5\n",
    "x = torch.randn( (batch_size, sequence_length, input_dim) )\n",
    "\n",
    "model = MultiHeadAttention(input_dim, d_model, num_heads)\n",
    "out = model.forward(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference \n",
    "1. https://colab.research.google.com/github/tensorflow/tensor2tensor/blob/master/tensor2tensor/notebooks/hello_t2t.ipynb\n",
    "2. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-agents",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
