{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab-1 : Attension\n",
    "\n",
    "Before, I address what is self-attension ? first we review how RNN maintaining a hidden state allows to incorporate its representation of previous words/vectors it has processed with the current one itâ€™s processing. Thus, Self attension is the method used by transformer for understanding the relevence of other words from vocabulary to the currect processing word in a given sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task-0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# python from scratch library\n",
    "import math\n",
    "import numpy as np \n",
    "\n",
    "# \n",
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task-1\n",
    "\n",
    "Attension - In a given sentence, It keep track every single word have some kind of affinity towards other word or not\n",
    "\n",
    "$$ \\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the query, key and value matrices\n",
    "def softmax(x):\n",
    "    return ( np.exp(x).T / np.sum(np.exp(x), axis=-1)).T\n",
    "\n",
    "def scaled_dot_product_attention( q, k, v, mask=None): # mask is none by default and is in encoder setting which \n",
    "                                                       # can be changed to decoder by passing mask\n",
    "    d_k=q.shape[-1]\n",
    "    scaled= np.matmul(q, k.T)/math.sqrt(d_k)\n",
    "    if mask is not None:\n",
    "        scaled= scaled+mask\n",
    "    print(\"scaled\", scaled)\n",
    "    attention= softmax(scaled)\n",
    "    out=np.matmul(attention, v)\n",
    "    return out, attention # new values , attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scaled [[0.87131859 0.86252614 0.48247449 0.6241726 ]\n",
      " [1.02132395 1.12710965 0.9030035  1.07739794]\n",
      " [1.02807672 0.91894661 0.71476049 0.99712315]\n",
      " [0.79079892 0.78990133 0.61787771 0.64059389]]\n",
      "Output:\n",
      " [[0.39125901 0.6638908  0.44900505 0.42719732 0.66951825 0.6359097\n",
      "  0.63514343 0.36880897]\n",
      " [0.41216041 0.69503973 0.4280375  0.400267   0.68886302 0.61062178\n",
      "  0.61598543 0.37990854]\n",
      " [0.40441745 0.67985271 0.43528608 0.42071218 0.68552592 0.6053806\n",
      "  0.62497903 0.38505464]\n",
      " [0.38840181 0.68091523 0.44167785 0.42328641 0.67909902 0.62552722\n",
      "  0.63767863 0.36375848]]\n",
      "Attention:\n",
      " [[0.2898456  0.28730832 0.19646893 0.22637715]\n",
      " [0.2464468  0.27394623 0.21894612 0.26066085]\n",
      " [0.27799762 0.24925649 0.20322146 0.26952442]\n",
      " [0.27020906 0.26996663 0.22730091 0.23252341]]\n",
      "scaled [[0.87131859       -inf       -inf       -inf]\n",
      " [1.02132395 1.12710965       -inf       -inf]\n",
      " [1.02807672 0.91894661 0.71476049       -inf]\n",
      " [0.79079892 0.78990133 0.61787771 0.64059389]]\n",
      "Output with mask:\n",
      " [[0.0417585  0.26769124 0.71248307 0.94398826 0.48935248 0.77218906\n",
      "  0.97777999 0.26722768]\n",
      " [0.33396451 0.49013788 0.55726389 0.49832409 0.54361588 0.82607508\n",
      "  0.68098118 0.3207578 ]\n",
      " [0.24872821 0.59618373 0.52908912 0.53328967 0.60396645 0.76228706\n",
      "  0.75989319 0.24462326]\n",
      " [0.38840181 0.68091523 0.44167785 0.42328641 0.67909902 0.62552722\n",
      "  0.63767863 0.36375848]]\n",
      "Attention with mask:\n",
      " [[1.         0.         0.         0.        ]\n",
      " [0.47357821 0.52642179 0.         0.        ]\n",
      " [0.38057073 0.34122495 0.27820432 0.        ]\n",
      " [0.27020906 0.26996663 0.22730091 0.23252341]]\n"
     ]
    }
   ],
   "source": [
    "# length of input sequence\n",
    "L=4\n",
    "# size of thes vectors\n",
    "d_k = 8\n",
    "d_v = 8\n",
    "# initialize the q k v matrices\n",
    "q = np.random.rand(L, d_k)\n",
    "k = np.random.rand(L, d_k)\n",
    "v = np.random.rand(L, d_v)\n",
    "\n",
    "# compute the attention\n",
    "out, attention = scaled_dot_product_attention(q, k, v)\n",
    "# print the output and attention\n",
    "print(\"Output:\\n\", out)\n",
    "print(\"Attention:\\n\", attention)\n",
    "\n",
    "# create a mask for the decoder - prediction of the future tokens\n",
    "mask= np.tril(np.ones((L,L)))\n",
    "mask[mask==0]=-np.inf\n",
    "mask[mask==1]=0\n",
    "\n",
    "# compute the attention with mask\n",
    "out_masked, attention_masked = scaled_dot_product_attention(q, k, v, mask)\n",
    "# print the output and attention with mask\n",
    "print(\"Output with mask:\\n\", out_masked)\n",
    "print(\"Attention with mask:\\n\", attention_masked)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task-2 : Multi-Head Attension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product(q, k, v, mask=None):\n",
    "    d_k = q.size()[-1]\n",
    "    scaled = torch.matmul(q, k.transpose(-1, -2)) / math.sqrt(d_k)\n",
    "    if mask is not None:\n",
    "        scaled += mask\n",
    "    attention = F.softmax(scaled, dim=-1)\n",
    "    values = torch.matmul(attention, v)\n",
    "    return values, attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_length = 4 # My name is Ankit\n",
    "batch_size = 1 # number of sentences in batch\n",
    "input_dim = 512 # vector dimension of each input\n",
    "d_model = 512 # vector dimension of each output  of each attention unit       \n",
    "x = torch.rand((batch_size, sequence_length, input_dim)) # input tensor\n",
    "\n",
    "qkv_layer = nn.Linear(input_dim, 3*d_model) # here we are concatenating q, k, v in one matrix which will be split later\n",
    "qkv = qkv_layer(x)\n",
    "\n",
    "num_heads= 8 # now we have 8 attention head\n",
    "head_dim= d_model//num_heads # each head will have dim of 512 / 8 => 64\n",
    "qkv = qkv.reshape(batch_size, sequence_length, num_heads, 3*head_dim)\n",
    "\n",
    "# let's change sequencing in qkv just to make things easy in future and perform parallel operations of last 2 dimensions\n",
    "qkv=qkv.permute(0, 2, 1, 3) # [batch_size, num_heads, sequence_length, 3*head_dim]\n",
    "qkv.shape\n",
    "\n",
    "# now we will divide the chuck of each head into q, k, v\n",
    "q, k, v=qkv.chunk(3, dim=-1) # divide last dimension by 3 i.e 192/3 = 64\n",
    "\n",
    "values, attention = scaled_dot_product(q, k, v)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, d_model, num_heads):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_model // num_heads\n",
    "        self.qkv_layer = nn.Linear(input_dim , 3 * d_model)\n",
    "        self.linear_layer = nn.Linear(d_model, d_model)\n",
    "    \n",
    "    def forward(self, x, mask=None): # forward pass\n",
    "        \n",
    "        batch_size, sequence_length, input_dim = x.size()\n",
    "        print(f\"x.size(): {x.size()}\\n\")\n",
    "        print(\"-\"*50)\n",
    "\n",
    "        qkv = self.qkv_layer(x)\n",
    "        print(f\"qkv.size(): {qkv.size()}\\n\")\n",
    "        print(\"-\"*50)\n",
    "\n",
    "        qkv = qkv.reshape(batch_size, sequence_length, self.num_heads, 3 * self.head_dim)\n",
    "        print(f\"qkv.size(): {qkv.size()}\")\n",
    "        qkv = qkv.permute(0, 2, 1, 3)\n",
    "        print(f\"qkv.size(): {qkv.size()}\\n\")\n",
    "        print(\"-\"*50)\n",
    "\n",
    "        q, k, v = qkv.chunk(3, dim=-1)\n",
    "        print(f\"q size: {q.size()}     k size: {k.size()}     v size: {v.size()}\\n\")\n",
    "        print(\"-\"*50)\n",
    "\n",
    "        values, attention = scaled_dot_product(q, k, v, mask)\n",
    "        print(f\"values.size(): {values.size()}          attention.size:{ attention.size()} \")\n",
    "        print(\"-\"*50)\n",
    "        values = values.reshape(batch_size, sequence_length, self.num_heads * self.head_dim)\n",
    "        print(f\"values.size(): {values.size()}\\n\")\n",
    "        print(\"-\"*50)\n",
    "\n",
    "        out = self.linear_layer(values)\n",
    "        print(f\"out.size(): {out.size()}\\n\")\n",
    "        print(\"-\"*50)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.size(): torch.Size([30, 5, 512])\n",
      "\n",
      "--------------------------------------------------\n",
      "qkv.size(): torch.Size([30, 5, 1536])\n",
      "\n",
      "--------------------------------------------------\n",
      "qkv.size(): torch.Size([30, 5, 8, 192])\n",
      "qkv.size(): torch.Size([30, 8, 5, 192])\n",
      "\n",
      "--------------------------------------------------\n",
      "q size: torch.Size([30, 8, 5, 64])     k size: torch.Size([30, 8, 5, 64])     v size: torch.Size([30, 8, 5, 64])\n",
      "\n",
      "--------------------------------------------------\n",
      "values.size(): torch.Size([30, 8, 5, 64])          attention.size:torch.Size([30, 8, 5, 5]) \n",
      "--------------------------------------------------\n",
      "values.size(): torch.Size([30, 5, 512])\n",
      "\n",
      "--------------------------------------------------\n",
      "out.size(): torch.Size([30, 5, 512])\n",
      "\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "input_dim = 512\n",
    "d_model = 512\n",
    "num_heads = 8\n",
    "\n",
    "batch_size = 30\n",
    "sequence_length = 5\n",
    "x = torch.randn( (batch_size, sequence_length, input_dim) )\n",
    "\n",
    "model = MultiHeadAttention(input_dim, d_model, num_heads)\n",
    "out = model.forward(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference \n",
    "1. https://colab.research.google.com/github/tensorflow/tensor2tensor/blob/master/tensor2tensor/notebooks/hello_t2t.ipynb\n",
    "2. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-agents",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
