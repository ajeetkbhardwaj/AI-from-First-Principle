{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyN7YQZSSZq3s4WQtcfF2eau"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["import keras\n","from keras import layers\n","from keras import ops\n","import numpy as np\n","\n","# Set a backend, e.g., \"tensorflow\", \"jax\", or \"torch\"\n","# Make sure to install the backend first, e.g., pip install tensorflow\n","import os\n","os.environ[\"KERAS_BACKEND\"] = \"tensorflow\""],"metadata":{"id":"npLd08QigzvH","executionInfo":{"status":"ok","timestamp":1753613572004,"user_tz":-330,"elapsed":4507,"user":{"displayName":"Ajeet Kumar","userId":"04620708898093466225"}}},"execution_count":1,"outputs":[]},{"cell_type":"code","execution_count":7,"metadata":{"id":"CALObnGRY9c0","executionInfo":{"status":"ok","timestamp":1753613978466,"user_tz":-330,"elapsed":3009,"user":{"displayName":"Ajeet Kumar","userId":"04620708898093466225"}}},"outputs":[],"source":["# Data parameters\n","# Define the number of samples you want for your small dataset\n","N_TRAIN_SAMPLES = 5000  # Example: Use 5000 training samples (out of 50,000)\n","N_TEST_SAMPLES = 1000   # Example: Use 1000 testing samples (out of 10,000)\n","\n","# Load the full CIFAR-10 dataset\n","(x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data()\n","\n","# --- Create the small subsets ---\n","# Ensure we don't request more samples than available\n","n_train_actual = min(N_TRAIN_SAMPLES, len(x_train))\n","n_test_actual = min(N_TEST_SAMPLES, len(x_test))\n","\n","x_train = x_train[:n_train_actual]\n","y_train = y_train[:n_train_actual]\n","x_test = x_test[:n_test_actual]\n","y_test = y_test[:n_test_actual]\n","\n","x_train = x_train.astype(\"float32\") / 255.0\n","y_train = keras.utils.to_categorical(y_train, 10)\n","x_test = x_test.astype(\"float32\") / 255.0\n","y_test = keras.utils.to_categorical(y_test, 10)\n"]},{"cell_type":"code","source":["# --- Model Definitions ---\n","\n","# Create a small student model\n","def create_student_model():\n","    inputs = keras.Input(shape=(32, 32, 3))\n","    x = layers.Conv2D(16, (3, 3), activation=\"relu\")(inputs)\n","    x = layers.MaxPooling2D((2, 2))(x)\n","    x = layers.Conv2D(32, (3, 3), activation=\"relu\")(x)\n","    x = layers.MaxPooling2D((2, 2))(x)\n","    x = layers.Flatten()(x)\n","    # The final layer has no activation, as we'll use from_logits=True in the loss\n","    outputs = layers.Dense(10)(x)\n","    return keras.Model(inputs=inputs, outputs=outputs, name=\"student\")\n","\n","# Create a larger teacher model\n","def create_teacher_model():\n","    inputs = keras.Input(shape=(32, 32, 3))\n","    x = layers.Conv2D(64, (3, 3), padding=\"same\", activation=\"relu\")(inputs)\n","    x = layers.Conv2D(64, (3, 3), padding=\"same\", activation=\"relu\")(x)\n","    x = layers.MaxPooling2D((2, 2))(x)\n","    x = layers.Conv2D(128, (3, 3), padding=\"same\", activation=\"relu\")(x)\n","    x = layers.Conv2D(128, (3, 3), padding=\"same\", activation=\"relu\")(x)\n","    x = layers.MaxPooling2D((2, 2))(x)\n","    x = layers.Flatten()(x)\n","    x = layers.Dense(256, activation=\"relu\")(x)\n","    # The final layer has no activation for consistency\n","    outputs = layers.Dense(10)(x)\n","    return keras.Model(inputs=inputs, outputs=outputs, name=\"teacher\")\n"],"metadata":{"id":"3lKINjeAgYRP","executionInfo":{"status":"ok","timestamp":1753613983463,"user_tz":-330,"elapsed":18,"user":{"displayName":"Ajeet Kumar","userId":"04620708898093466225"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["# --- Train the Teacher Model (as a baseline) ---\n","print(\"--- Training the Teacher Model ---\")\n","teacher = create_teacher_model()\n","teacher.compile(\n","    optimizer=\"adam\",\n","    loss=keras.losses.CategoricalCrossentropy(from_logits=True),\n","    metrics=[\"accuracy\"],\n",")\n","teacher.fit(x_train, y_train, epochs=10, batch_size=64, validation_split=0.1)\n","teacher_test_loss, teacher_test_acc = teacher.evaluate(x_test, y_test)\n","print(f\"Teacher Test Accuracy: {teacher_test_acc:.4f}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GQwqTHzJgcGm","executionInfo":{"status":"ok","timestamp":1753614772031,"user_tz":-330,"elapsed":785421,"user":{"displayName":"Ajeet Kumar","userId":"04620708898093466225"}},"outputId":"d02b473f-52ae-4dd8-ec41-8bfad00fb9fd"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["--- Training the Teacher Model ---\n","Epoch 1/10\n","\u001b[1m71/71\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 969ms/step - accuracy: 0.1649 - loss: 2.1832 - val_accuracy: 0.3280 - val_loss: 1.8638\n","Epoch 2/10\n","\u001b[1m71/71\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 953ms/step - accuracy: 0.3531 - loss: 1.7605 - val_accuracy: 0.4180 - val_loss: 1.6884\n","Epoch 3/10\n","\u001b[1m71/71\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 981ms/step - accuracy: 0.4407 - loss: 1.5387 - val_accuracy: 0.4620 - val_loss: 1.5323\n","Epoch 4/10\n","\u001b[1m71/71\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 955ms/step - accuracy: 0.4940 - loss: 1.3925 - val_accuracy: 0.4360 - val_loss: 1.5905\n","Epoch 5/10\n","\u001b[1m71/71\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m67s\u001b[0m 950ms/step - accuracy: 0.5326 - loss: 1.2747 - val_accuracy: 0.5040 - val_loss: 1.4126\n","Epoch 6/10\n","\u001b[1m71/71\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 954ms/step - accuracy: 0.6028 - loss: 1.1095 - val_accuracy: 0.5240 - val_loss: 1.3450\n","Epoch 7/10\n","\u001b[1m71/71\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 951ms/step - accuracy: 0.6718 - loss: 0.9188 - val_accuracy: 0.5240 - val_loss: 1.4815\n","Epoch 8/10\n","\u001b[1m71/71\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 960ms/step - accuracy: 0.7404 - loss: 0.7579 - val_accuracy: 0.5540 - val_loss: 1.5041\n","Epoch 9/10\n","\u001b[1m71/71\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 963ms/step - accuracy: 0.7931 - loss: 0.5995 - val_accuracy: 0.5640 - val_loss: 1.5318\n","Epoch 10/10\n","\u001b[1m71/71\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 961ms/step - accuracy: 0.8682 - loss: 0.3801 - val_accuracy: 0.5220 - val_loss: 1.9162\n","\u001b[1m32/32\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 119ms/step - accuracy: 0.5486 - loss: 1.7557\n","Teacher Test Accuracy: 0.5490\n"]}]},{"cell_type":"code","source":["class Distiller(keras.Model):\n","    def __init__(self, student, teacher):\n","        super().__init__()\n","        self.student = student\n","        self.teacher = teacher\n","        # We will track these losses manually\n","        self.student_loss_tracker = keras.metrics.Mean(name=\"student_loss\")\n","        self.distillation_loss_tracker = keras.metrics.Mean(name=\"distillation_loss\")\n","\n","    @property\n","    def metrics(self):\n","        # We list our `Metric` objects here so `reset_states()` can be called automatically.\n","        return [\n","            self.student_loss_tracker,\n","            self.distillation_loss_tracker,\n","        ]\n","\n","    def compile(\n","        self, optimizer, metrics, student_loss_fn, distillation_loss_fn, alpha=0.1, temperature=3\n","    ):\n","        super().compile(optimizer=optimizer, metrics=metrics)\n","        self.student_loss_fn = student_loss_fn\n","        self.distillation_loss_fn = distillation_loss_fn\n","        self.alpha = alpha\n","        self.temperature = temperature\n","\n","    def train_step(self, data):\n","        x, y = data\n","\n","        # Get teacher predictions\n","        teacher_predictions = self.teacher(x, training=False)\n","\n","        with keras.backend.GradientTape() as tape:\n","            # Get student predictions\n","            student_predictions = self.student(x, training=True)\n","\n","            # 1. Calculate the student loss (against hard labels)\n","            student_loss = self.student_loss_fn(y, student_predictions)\n","\n","            # 2. Calculate the distillation loss (against soft teacher labels)\n","            distillation_loss = self.distillation_loss_fn(\n","                ops.softmax(teacher_predictions / self.temperature),\n","                ops.softmax(student_predictions / self.temperature),\n","            )\n","\n","            # 3. Combine the losses\n","            total_loss = self.alpha * student_loss + (1 - self.alpha) * distillation_loss\n","\n","        # Compute gradients of the total loss w.r.t the student's trainable variables\n","        trainable_vars = self.student.trainable_variables\n","        gradients = tape.gradient(total_loss, trainable_vars)\n","\n","        # Update student weights\n","        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n","\n","        # Update metrics\n","        self.student_loss_tracker.update_state(student_loss)\n","        self.distillation_loss_tracker.update_state(distillation_loss)\n","        # Note: Compiled metrics are updated automatically\n","        self.compute_metrics(x, y, student_predictions, None)\n","\n","        # Return a dict of performance\n","        results = {m.name: m.result() for m in self.metrics}\n","        return results\n","\n","    def test_step(self, data):\n","        # The test step only evaluates the student model on the validation data\n","        x, y = data\n","        student_predictions = self.student(x, training=False)\n","        student_loss = self.student_loss_fn(y, student_predictions)\n","        self.student_loss_tracker.update_state(student_loss)\n","        self.compute_metrics(x, y, student_predictions, None)\n","        results = {m.name: m.result() for m in self.metrics}\n","        return results\n","\n"],"metadata":{"id":"q_Yed342iwIf","executionInfo":{"status":"ok","timestamp":1753614932292,"user_tz":-330,"elapsed":5,"user":{"displayName":"Ajeet Kumar","userId":"04620708898093466225"}}},"execution_count":16,"outputs":[]},{"cell_type":"code","source":["# --- Distillation using Way 1 ---\n","print(\"\\n--- Distilling with Way 1: Model Subclassing ---\")\n","student_v1 = create_student_model()\n","distiller = Distiller(student=student_v1, teacher=teacher)\n","\n","distiller.compile(\n","    optimizer=\"adam\",\n","    metrics=[\"accuracy\"],\n","    student_loss_fn=keras.losses.CategoricalCrossentropy(from_logits=True),\n","    distillation_loss_fn=keras.losses.KLDivergence(),\n","    alpha=0.1,\n","    temperature=10,\n",")\n","\n","distiller.fit(x_train, y_train, epochs=5, batch_size=64)\n","student_v1_loss, student_v1_acc = student_v1.evaluate(x_test, y_test)\n","print(f\"Way 1 - Distilled Student Accuracy: {student_v1_acc:.4f}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":408},"id":"m59tCWlblqX_","executionInfo":{"status":"error","timestamp":1753614941916,"user_tz":-330,"elapsed":314,"user":{"displayName":"Ajeet Kumar","userId":"04620708898093466225"}},"outputId":"63f05798-9c34-4659-d574-d679ee22faa5"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","--- Distilling with Way 1: Model Subclassing ---\n","Epoch 1/5\n"]},{"output_type":"error","ename":"AttributeError","evalue":"module 'keras.api.backend' has no attribute 'GradientTape'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-17-2694055327.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m )\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mdistiller\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0mstudent_v1_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstudent_v1_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstudent_v1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Way 1 - Distilled Student Accuracy: {student_v1_acc:.4f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;31m# `keras.config.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipython-input-16-3183845629.py\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mteacher_predictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mteacher\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGradientTape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m             \u001b[0;31m# Get student predictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0mstudent_predictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstudent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: module 'keras.api.backend' has no attribute 'GradientTape'"]}]},{"cell_type":"markdown","source":["This approach is more idiomatic for many Keras users. It avoids subclassing Model by cleverly constructing a training model and a custom loss function.\n","Concept:\n","Create a temporary \"training model\" using the Functional API that takes an input x and outputs both the student's and teacher's predictions.\n","Write a custom loss function that accepts this dual output (y_pred) and calculates the combined distillation loss.\n","Train this temporary model. The original student model's weights will be updated because it's part of the graph.\n","For evaluation, use the original student model."],"metadata":{"id":"O24JaegGjp7N"}},{"cell_type":"code","source":["def distillation_loss(y_true, y_pred, alpha=0.1, temperature=10):\n","    # y_pred will be a list of [student_logits, teacher_logits]\n","    student_logits, teacher_logits = y_pred\n","\n","    # 1. Student loss against the hard labels\n","    student_loss = keras.losses.categorical_crossentropy(y_true, student_logits, from_logits=True)\n","\n","    # 2. Distillation loss against the soft teacher labels\n","    distillation_loss = keras.losses.kl_divergence(\n","        ops.softmax(teacher_logits / temperature),\n","        ops.softmax(student_logits / temperature),\n","    )\n","\n","    # 3. Combine the two losses\n","    total_loss = alpha * student_loss + (1 - self.alpha) * distillation_loss\n","    return total_loss"],"metadata":{"id":"CVOxeagtl4YQ","executionInfo":{"status":"ok","timestamp":1753614882011,"user_tz":-330,"elapsed":6,"user":{"displayName":"Ajeet Kumar","userId":"04620708898093466225"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["# --- Distillation using Way 2 ---\n","print(\"\\n--- Distilling with Way 2: Functional API + Custom Loss ---\")\n","student_v2 = create_student_model()\n","\n","# Make the teacher non-trainable\n","teacher.trainable = False\n","\n","# Create the combined training model\n","inputs = keras.Input(shape=(32, 32, 3))\n","student_output = student_v2(inputs)\n","teacher_output = teacher(inputs)\n","\n","# The training model outputs a list of [student, teacher] predictions\n","training_model = keras.Model(inputs, [student_output, teacher_output])\n","\n","# Compile the training model with our custom loss\n","training_model.compile(\n","    optimizer=\"adam\",\n","    loss=distillation_loss, # Our custom loss function\n","    metrics=[\"accuracy\"],   # This metric will be calculated on the first output (student)\n",")\n","\n","# Train the model. The loss function gets y_true and the model's output [s_out, t_out].\n","training_model.fit(x_train, y_train, epochs=5, batch_size=64)\n","\n","# Evaluate the final student model directly\n","student_v2_loss, student_v2_acc = student_v2.evaluate(x_test, y_test)\n","print(f\"Way 2 - Distilled Student Accuracy: {student_v2_acc:.4f}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":460},"id":"ZeNNH5lii62G","executionInfo":{"status":"error","timestamp":1753614887027,"user_tz":-330,"elapsed":513,"user":{"displayName":"Ajeet Kumar","userId":"04620708898093466225"}},"outputId":"eebfc826-5fcc-4219-e1c2-2efce6a21bec"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","--- Distilling with Way 2: Functional API + Custom Loss ---\n","Epoch 1/5\n"]},{"output_type":"error","ename":"KeyError","evalue":"\"The path: (0,) in the `loss` argument, can't be found in either the model's output (`y_pred`) or in the labels (`y_true`).\"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-15-1719048603.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;31m# Train the model. The loss function gets y_true and the model's output [s_out, t_out].\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0mtraining_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;31m# Evaluate the final student model directly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;31m# `keras.config.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/trainers/compile_utils.py\u001b[0m in \u001b[0;36m_build_nested\u001b[0;34m(self, y_true, y_pred, loss, output_names, current_path)\u001b[0m\n\u001b[1;32m    551\u001b[0m                 \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mkey_check_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 553\u001b[0;31m                 raise KeyError(\n\u001b[0m\u001b[1;32m    554\u001b[0m                     \u001b[0;34mf\"The path: {current_path + (key,)} in \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    555\u001b[0m                     \u001b[0;34m\"the `loss` argument, can't be found in \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyError\u001b[0m: \"The path: (0,) in the `loss` argument, can't be found in either the model's output (`y_pred`) or in the labels (`y_true`).\""]}]},{"cell_type":"markdown","source":["This is the most modern and concise Keras 3 pattern. It leverages the add_loss() method to \"bake\" the distillation loss directly into the model's graph.\n","Concept:\n","\n","Define the model using the Functional API.\n","Within the model definition, calculate the distillation loss (KL divergence between student and teacher logits).\n","Add this loss directly to the model using model.add_loss().\n","Compile the model with only the standard student loss (e.g., Cross-Entropy). Keras will automatically combine the compile loss and the added loss during training.\n","\n"],"metadata":{"id":"qm92DdmAjgS4"}},{"cell_type":"code","source":["# --- Distillation using Way 3 ---\n","print(\"\\n--- Distilling with Way 3: Functional API + add_loss() ---\")\n","\n","# Hyperparameters\n","alpha = 0.1\n","temperature = 10\n","\n","# Make the teacher non-trainable again\n","teacher.trainable = False\n","\n","# Create the student model via the Functional API\n","student_v3 = create_student_model()\n","inputs = student_v3.input\n","student_logits = student_v3.output\n","\n","# Get teacher logits for the same input\n","teacher_logits = teacher(inputs)\n","\n","# Calculate the distillation loss\n","distillation_loss = keras.losses.kl_divergence(\n","    ops.softmax(teacher_logits / temperature),\n","    ops.softmax(student_logits / temperature),\n",")\n","\n","# Create the final distiller model that includes the added loss\n","# The model takes standard inputs and produces student logits as output\n","distiller_model = keras.Model(inputs, student_logits)\n","distiller_model.add_loss((1 - alpha) * ops.mean(distillation_loss)) # Note the weighting\n","\n","# Compile the model with the standard student loss\n","distiller_model.compile(\n","    optimizer=\"adam\",\n","    # The loss is weighted by alpha here\n","    loss=keras.losses.CategoricalCrossentropy(from_logits=True, weight=alpha),\n","    metrics=[\"accuracy\"],\n",")\n","\n","# Fit the distiller model. Keras handles both losses automatically.\n","distiller_model.fit(x_train, y_train, epochs=5, batch_size=64)\n","\n","# The distiller_model *is* our final student model.\n","student_v3_loss, student_v3_acc = distiller_model.evaluate(x_test, y_test)\n","print(f\"Way 3 - Distilled Student Accuracy: {student_v3_acc:.4f}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":402},"id":"9H9DMUhZjI9v","executionInfo":{"status":"error","timestamp":1753614948434,"user_tz":-330,"elapsed":76,"user":{"displayName":"Ajeet Kumar","userId":"04620708898093466225"}},"outputId":"dff74a03-38db-496b-b13f-8f2d0205db4d"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","--- Distilling with Way 3: Functional API + add_loss() ---\n"]},{"output_type":"error","ename":"NotImplementedError","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-18-3416556285.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;31m# The model takes standard inputs and produces student logits as output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0mdistiller_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstudent_logits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m \u001b[0mdistiller_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdistillation_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Note the weighting\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;31m# Compile the model with the standard student loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/models/functional.py\u001b[0m in \u001b[0;36madd_loss\u001b[0;34m(self, loss)\u001b[0m\n\u001b[1;32m    334\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0madd_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m         \u001b[0;31m# Symbolic only. TODO\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 336\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    337\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNotImplementedError\u001b[0m: "]}]},{"cell_type":"markdown","source":["### Step-N ğŸ‡°\n","1. [Knowledge Distillation using Keras](https://keras.io/examples/vision/knowledge_distillation/)"],"metadata":{"id":"4srXuirOkpv3"}},{"cell_type":"code","source":[],"metadata":{"id":"Ej3ydOeFkxHP"},"execution_count":null,"outputs":[]}]}