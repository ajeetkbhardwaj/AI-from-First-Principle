{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyOWTp4+7tLxCNz5OhKFV2Ec"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["Knowledge Distillation is a procedure for model compression, in which a small (student) model is trained to match a large pre-trained (teacher) model. Knowledge is transferred from the teacher model to the student by minimizing a loss function, aimed at matching softened teacher logits as well as ground-truth labels. The logits are softened by applying a \"temperature\" scaling function in the softmax, effectively smoothing out the probability distribution and revealing inter-class relationships learned by the teacher.\n","\n","\n","How to distil a fine-tuned ViT model(teacher model to the MobileNet Model(student model) suing Trainer API of transformer ?"],"metadata":{"id":"ROnXCg2vZl9Z"}},{"cell_type":"markdown","source":["### Step-0 :"],"metadata":{"id":"fVxM_98oZKKv"}},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"mGm2U_7CYxL0","executionInfo":{"status":"ok","timestamp":1753611750514,"user_tz":-330,"elapsed":103930,"user":{"displayName":"Ajeet Kumar","userId":"04620708898093466225"}},"outputId":"e694cd8e-683d-40ea-cc23-9908fd848c14"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.7/41.7 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.2/11.2 MB\u001b[0m \u001b[31m129.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m117.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m558.8/558.8 kB\u001b[0m \u001b[31m47.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m125.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m93.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m64.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m848.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m82.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","tensorflow 2.18.0 requires tensorboard<2.19,>=2.18, but you have tensorboard 2.20.0 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0m"]}],"source":["!pip install transformers datasets accelerate tensorboard evaluate --upgrade --quiet"]},{"cell_type":"code","source":["from datasets import load_dataset\n","from transformers import AutoImageProcessor\n","\n","from transformers import TrainingArguments, Trainer\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from accelerate.test_utils.testing import get_backend\n","\n","from transformers import AutoModelForImageClassification, MobileNetV2Config, MobileNetV2ForImageClassification\n","\n","import evaluate\n","import numpy as np\n","\n","from transformers import DefaultDataCollator"],"metadata":{"id":"TM2wRt39bJ95"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# login the the huggingface hub using hf token for ush our model to the Hugging Face Hub through the Trainer\n","from huggingface_hub import notebook_login\n","\n","notebook_login()"],"metadata":{"id":"u_FXkaO9dHSW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Step-1\n","\n","1. [Teacher model](https://huggingface.co/merve/beans-vit-224) is the fine-tuned form of the [original model](https://huggingface.co/google/vit-base-patch16-224-in21k) on the beans dataset from the datasets hf-library.\n","\n","We will distill teacher model to randomly initialized MobileNetV2 model.\n","\n","Note : both models are trained for the image classification task."],"metadata":{"id":"wzLWWvNnZNpn"}},{"cell_type":"code","source":["# load the beans dataset\n","dataset = load_dataset(\"beans\")\n","# load the pretrained fineturned Teacher model\n","teacher_processor = AutoImageProcessor.from_pretrained(\"merve/beans-vit-224\")\n","# image preprocessor from the model, return same output with same resolution\n","def process(examples):\n","    processed_inputs = teacher_processor(examples[\"image\"])\n","    return processed_inputs\n","# we apply the preprocessing to the every split of dataset using map()\n","processed_datasets = dataset.map(process, batched=True)"],"metadata":{"id":"DYuKb8gvZOJ_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Aim : student model able to mimic the teacher model.\n","\n","1. get the logits output from the teacher and the student.\n","2. divide each of them by the parameter temperature which controls the importance of each soft target.\n","3. the Kullback-Leibler Divergence loss to compute the divergence between the student and teacher.\n","\n","\n","Note :\n","- A parameter called lambda weighs the importance of the distillation loss. Example - `temprature=5` and `lambda=0.5`.\n","- Given two data P and Q, KL Divergence explains how much extra information we need to represent P using Q. If two are identical, their KL divergence is zero, as there’s no other information needed to explain P from Q. Thus, in the context of knowledge distillation, KL divergence is useful.\n"],"metadata":{"id":"xg-F3XmocBZl"}},{"cell_type":"code","source":["class ImageDistilTrainer(Trainer):\n","    def __init__(self, teacher_model=None, student_model=None, temperature=None, lambda_param=None,  *args, **kwargs):\n","        super().__init__(model=student_model, *args, **kwargs)\n","        self.teacher = teacher_model\n","        self.student = student_model\n","        self.loss_function = nn.KLDivLoss(reduction=\"batchmean\")\n","        device, _, _ = get_backend() # automatically detects the underlying device type (CUDA, CPU, XPU, MPS, etc.)\n","        self.teacher.to(device)\n","        self.teacher.eval()\n","        self.temperature = temperature\n","        self.lambda_param = lambda_param\n","\n","    def compute_loss(self, student, inputs, return_outputs=False):\n","        student_output = self.student(**inputs)\n","\n","        with torch.no_grad():\n","          teacher_output = self.teacher(**inputs)\n","\n","        # Compute soft targets for teacher and student\n","        soft_teacher = F.softmax(teacher_output.logits / self.temperature, dim=-1)\n","        soft_student = F.log_softmax(student_output.logits / self.temperature, dim=-1)\n","\n","        # Compute the loss\n","        distillation_loss = self.loss_function(soft_student, soft_teacher) * (self.temperature ** 2)\n","\n","        # Compute the true label loss\n","        student_target_loss = student_output.loss\n","\n","        # Calculate final loss\n","        loss = (1. - self.lambda_param) * student_target_loss + self.lambda_param * distillation_loss\n","        return (loss, student_output) if return_outputs else loss"],"metadata":{"id":"yu1QI3nFc3so"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["training_args = TrainingArguments(\n","    output_dir=\"my-awesome-model\",\n","    num_train_epochs=30,\n","    fp16=True,\n","    logging_dir=f\"{repo_name}/logs\",\n","    logging_strategy=\"epoch\",\n","    eval_strategy=\"epoch\",\n","    save_strategy=\"epoch\",\n","    load_best_model_at_end=True,\n","    metric_for_best_model=\"accuracy\",\n","    report_to=\"tensorboard\",\n","    push_to_hub=True,\n","    hub_strategy=\"every_save\",\n","    hub_model_id=repo_name,\n","    )\n","\n","num_labels = len(processed_datasets[\"train\"].features[\"labels\"].names)\n","\n","# initialize models\n","teacher_model = AutoModelForImageClassification.from_pretrained(\n","    \"merve/beans-vit-224\",\n","    num_labels=num_labels,\n","    ignore_mismatched_sizes=True\n",")\n","\n","# training MobileNetV2 from scratch\n","student_config = MobileNetV2Config()\n","student_config.num_labels = num_labels\n","student_model = MobileNetV2ForImageClassification(student_config)"],"metadata":{"id":"e7OnAEEodbd9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# compute_metrics function to evaluate our model on the test set.\n","# it is used during the training process to compute the accuracy & f1 of our model.\n","\n","accuracy = evaluate.load(\"accuracy\")\n","\n","def compute_metrics(eval_pred):\n","    predictions, labels = eval_pred\n","    acc = accuracy.compute(references=labels, predictions=np.argmax(predictions, axis=1))\n","    return {\"accuracy\": acc[\"accuracy\"]}"],"metadata":{"id":"gB49bICudhB-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Initialize the Trainer with the training arguments and data collator.\n","data_collator = DefaultDataCollator()\n","trainer = ImageDistilTrainer(\n","    student_model=student_model,\n","    teacher_model=teacher_model,\n","    training_args=training_args,\n","    train_dataset=processed_datasets[\"train\"],\n","    eval_dataset=processed_datasets[\"validation\"],\n","    data_collator=data_collator,\n","    processing_class=teacher_processor,\n","    compute_metrics=compute_metrics,\n","    temperature=5,\n","    lambda_param=0.5\n",")"],"metadata":{"id":"Vz1YruUZd0fG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# training our model\n","trainer.train()"],"metadata":{"id":"DjSPw9Kfd8yO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# evaluation of our model on the test dataset\n","trainer.evaluate(processed_dataset[\"tests\"])"],"metadata":{"id":"O6iztmLNeAg2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Step : References\n","1.[Knowledge Distillation for Computer vision](https://huggingface.co/docs/transformers/en/tasks/knowledge_distillation_for_image_classification)"],"metadata":{"id":"8en9-Uj4Y-bv"}}]}