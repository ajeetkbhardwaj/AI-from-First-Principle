{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPPphIzOrAN8nm4YS/5qxH9"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["1. **Define teacher and student models**  \n","   Typically, the teacher is a large pretrained model, and the student is a smaller model.\n","\n","2. **Softened output with temperature**  \n","   Use a temperature $T > 1 $to soften the logits before softmax for both teacher and student:\n","   $$\n","   q_i = \\frac{\\exp(z_i/T)}{\\sum_j \\exp(z_j/T)}\n","   $$\n","   where $z_i $are logits.\n","\n","3. **Loss function**  \n","   Combine:\n","   - Cross-entropy loss with true labels for the student\n","   - KL divergence loss between teacher and student softened outputs\n","\n","   Loss could be:\n","   $$\n","   L = \\alpha \\cdot CE(y, \\hat{y}_s) + (1 - \\alpha) \\cdot T^2 \\cdot KL(q_t, q_s)\n","   $$\n","   where $\\alpha $balances the two losses, $y $is true label, $\\hat{y}_s $student output, $q_t $teacher softened output, $q_s $student softened output.\n","\n","4. **Training loop**  \n","   For each batch:\n","   - Get teacher outputs (no gradient needed)\n","   - Get student outputs\n","   - Compute distillation loss\n","   - Backpropagate and optimize student model"],"metadata":{"id":"0n4cciGfm0tx"}},{"cell_type":"markdown","source":["### Step-0 :\n"],"metadata":{"id":"JQJjoBKMVLgI"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"xt9YxpUVVFms"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torchvision.transforms as transforms\n","import torchvision.datasets as datasets\n"]},{"cell_type":"code","source":["# let check and device to gpu if available\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(device)"],"metadata":{"id":"rFBkA8-jVQgW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Step-Last :\n","0. [Knowledge Distillation: Principles, Algorithms, Applications](https://neptune.ai/blog/knowledge-distillation)\n","1. [Knowledge Distillation for Beginners using PyTorch](https://docs.pytorch.org/tutorials/beginner/knowledge_distillation_tutorial.html)\n","2. [KN-Pytorch part-1](https://www.kaggle.com/code/shivangitomar/knowledge-distillation-part-1-pytorch)\n","3. [KN-PyTorch part-2](https://www.kaggle.com/code/shivangitomar/knowledge-distillation-part-2-pytorch/)\n","4. [KD-PyTorch github benchmarks](https://github.com/haitongli/knowledge-distillation-pytorch)\n","5[Youtube - KD](https://youtu.be/l44uC7jfnvY?feature=shared)"],"metadata":{"id":"dkgfjxMeWten"}},{"cell_type":"code","source":[],"metadata":{"id":"ZGP6TF0OXr0Q"},"execution_count":null,"outputs":[]}]}