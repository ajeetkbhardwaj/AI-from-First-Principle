# Parameter-Efficient Fine-Tuning (PEFT)

What is PEFT ?

- A set of advanced techniques designed to address these challenges by reducing the number of trainable parameters required for fine-tuning LLMs.
- It enables efficient adaptation of pre-trained models to specific tasks while significantly lowering memory requirements and training time.
- It maintain the performance levels close to those achieved by full fine-tuning.

When and where should we use the PEFT ?

- PEFT  is valuable, where resources are limited or multiple models needed to be finetuned for different tasks.

- They allows engineers to leverage the power of large pre-trained models without the associated computational overhead.

Why use PEFT ?

We use the PEFT for finetuning because of the following reasone

- Resource Efficiency - It reduces the memory footprint and computational cost associated with finetuning large models
- Scalability - it feasible to fine-tune multiple models for different tasks without requiring extensive hardware resources.
- Flexibility - It can adapted to a wide range of NLP tasks, providing a verstile toolkit for model finetuning
