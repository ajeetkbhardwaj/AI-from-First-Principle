{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3af87cb4",
   "metadata": {},
   "source": [
    "# W6 : Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aae5565a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a0cd99",
   "metadata": {},
   "source": [
    "Naive Bayes is a **generative probabilistic model**. Let’s assume we have a dataset sampled from a **Bernoulli experiment**:\n",
    "\n",
    "$$\n",
    "\\mathcal{D} = {(x_i, y_i) }_{i=1}^n, \\quad x_i \\in {0,1}^m, \\ y_i \\in {1,\\dots,K}\n",
    "$$\n",
    "\n",
    "Then:\n",
    "\n",
    "1. **Naive Bayes assumption**: each feature is independent given the class (y):\n",
    "\n",
    "$$\n",
    "P(x \\mid y=c) = \\prod_{j=1}^m P(x_j \\mid y=c)\n",
    "$$\n",
    "\n",
    "2. **Bernoulli distribution**: each feature follows a Bernoulli distribution conditioned on the class:\n",
    "\n",
    "$$\n",
    "P(x_j = 1 \\mid y=c) = \\theta_{jc}, \\quad P(x_j = 0 \\mid y=c) = 1 - \\theta_{jc}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "af1f2c44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 2) (4,)\n",
      "(6, 2) (6,)\n"
     ]
    }
   ],
   "source": [
    "# binary class classification task dataset\n",
    "X1 = np.array([[1,0], [0,1], [0,1], [1,0]])\n",
    "y1 = np.array([1, 0, 0, 1])\n",
    "print(X1.shape, y1.shape)\n",
    "# multi-class classification task dataset\n",
    "X2 = np.array([[1,0], [0,1], [0,1], [1,0], [1, 1], [1, 1]])\n",
    "y2 = np.array([1, 0, 0, 1, 2, 2])\n",
    "print(X2.shape, y2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbcce09e",
   "metadata": {},
   "source": [
    "For class $(c)$, let $N_c$ be the number of samples with $y_i = c$. Using **Laplace smoothing** $\\alpha>0$, the **conditional probability** of feature $j$ is:\n",
    "\n",
    "$$\n",
    "\\theta_{jc} = P(x_j=1 \\mid y=c) = \\frac{\\sum_{i:y_i=c} x_{ij} + \\alpha}{N_c + 2\\alpha}\n",
    "$$\n",
    "\n",
    "The **class prior** is estimated as:\n",
    "\n",
    "$$\n",
    "\\pi_c = P(y=c) = \\frac{N_c + \\alpha}{n + K \\alpha}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2efa81cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------Binary Classification----------\n",
      "Class conditional density : [[0.04545455 0.95454545]\n",
      " [0.95454545 0.04545455]]\n",
      "Prior probabilities of class : [0.5 0.5]\n",
      "-------------multi-class classification-----------\n",
      "Class conditional density : [[0.04545455 0.95454545]\n",
      " [0.95454545 0.04545455]\n",
      " [0.95454545 0.95454545]]\n",
      "Prior probabilities of class : [0.33333333 0.33333333 0.33333333]\n"
     ]
    }
   ],
   "source": [
    "class NaiveBayesBernoulli(object):\n",
    "    def __init__(self, alpha=0.1):\n",
    "        self.alpha = alpha\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Task : Estimation of parameters of bernoulli distribution by naive bayes\n",
    "        Inputs :\n",
    "            X: Feature matrix of shape (n,m)\n",
    "            y: Label vector of shape (n,) \n",
    "        Outputs:\n",
    "            w_{j y_c}, w_prior\n",
    "        \"\"\"\n",
    "        n_samples, m_features = X.shape\n",
    "        n_classes = len(np.unique(y))\n",
    "\n",
    "        # Inialization of weight matrix\n",
    "        self.W = np.zeros((n_classes,m_features), dtype=np.float64)\n",
    "        self.W_prior = np.zeros(n_classes, dtype=np.float64)\n",
    "\n",
    "        # Processing samples for each classes seperatly\n",
    "        for c in range(n_classes):\n",
    "            # 1. find example with label c\n",
    "            X_c = X[y==c]\n",
    "            # 2. Estimation of W{j, y_c} = P(x_j | y_c = c)\n",
    "            self.W[c, :] = (np.sum(X_c, axis=0) + self.alpha)/(X_c.shape[0] + 2.0 * self.alpha)\n",
    "            # 3. Estimation of prior\n",
    "            self.W_prior[c] = (X_c.shape[0] + self.alpha)/(float(n_samples) + n_classes * self.alpha)\n",
    "        \n",
    "        return self.W, self.W_prior\n",
    "\n",
    "model = NaiveBayesBernoulli()\n",
    "print(\"--------------Binary Classification----------\")\n",
    "class_cond_density, prior = model.fit(X1, y1)\n",
    "print(f\"Class conditional density : {class_cond_density}\")\n",
    "print(f\"Prior probabilities of class : {prior}\")\n",
    "\n",
    "print(\"-------------multi-class classification-----------\")\n",
    "class_cond_density, prior = model.fit(X2, y2)\n",
    "print(f\"Class conditional density : {class_cond_density}\")\n",
    "print(f\"Prior probabilities of class : {prior}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f49514b",
   "metadata": {},
   "source": [
    "\n",
    "For a single sample $x$, the **log-likelihood of class (c)** is:\n",
    "\n",
    "$$\n",
    "\\log P(x \\mid y=c) + \\log P(y=c)\n",
    "= \\sum_{j=1}^m \\big$$ x_j \\log \\theta_{jc} + (1-x_j) \\log(1-\\theta_{jc}) \\big$$ + \\log \\pi_c\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bd6b47b0",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NaiveBayesBernoulli' object has no attribute 'W'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 35\u001b[39m\n\u001b[32m     32\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m X @ (np.log(\u001b[38;5;28mself\u001b[39m.W).T) + (\u001b[32m1\u001b[39m - X) @ (np.log(\u001b[32m1\u001b[39m-\u001b[38;5;28mself\u001b[39m.W).T) + np.log(\u001b[38;5;28mself\u001b[39m.W_prior)\n\u001b[32m     34\u001b[39m model = NaiveBayesBernoulli()       \n\u001b[32m---> \u001b[39m\u001b[32m35\u001b[39m log_likelihood_prior = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlog_likelihood_prior_probability\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX1\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     36\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m--------------Binary Classification----------\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     37\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mlog likelihood of Prior probabilities of class : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlog_likelihood_prior\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 32\u001b[39m, in \u001b[36mNaiveBayesBernoulli.log_likelihood_prior_probability\u001b[39m\u001b[34m(self, X)\u001b[39m\n\u001b[32m     31\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mlog_likelihood_prior_probability\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m X @ (np.log(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mW\u001b[49m).T) + (\u001b[32m1\u001b[39m - X) @ (np.log(\u001b[32m1\u001b[39m-\u001b[38;5;28mself\u001b[39m.W).T) + np.log(\u001b[38;5;28mself\u001b[39m.W_prior)\n",
      "\u001b[31mAttributeError\u001b[39m: 'NaiveBayesBernoulli' object has no attribute 'W'"
     ]
    }
   ],
   "source": [
    "class NaiveBayesBernoulli(object):\n",
    "    def __init__(self, alpha=0.1):\n",
    "        self.alpha = alpha\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Task : Estimation of parameters of bernoulli distribution by naive bayes\n",
    "        Inputs :\n",
    "            X: Feature matrix of shape (n,m)\n",
    "            y: Label vector of shape (n,) \n",
    "        Outputs:\n",
    "            w_{j y_c}, w_prior\n",
    "        \"\"\"\n",
    "        n_samples, m_features = X.shape\n",
    "        n_classes = len(np.unique(y))\n",
    "\n",
    "        # Inialization of weight matrix\n",
    "        self.W = np.zeros((n_classes,m_features), dtype=np.float64)\n",
    "        self.W_prior = np.zeros(n_classes, dtype=np.float64)\n",
    "\n",
    "        # Processing samples for each classes seperatly\n",
    "        for c in range(n_classes):\n",
    "            # 1. find example with label c\n",
    "            X_c = X[y==c]\n",
    "            # 2. Estimation of W{j, y_c} = P(x_j | y_c = c)\n",
    "            self.W[c, :] = (np.sum(X_c, axis=0) + self.alpha)/(X_c.shape[0] + 2.0 * self.alpha)\n",
    "            # 3. Estimation of prior\n",
    "            self.W_priors[c] = (X_c.shape[0] + self.alpha)/(float(n_samples) + n_classes * self.alpha)\n",
    "        \n",
    "        return self.W, self.W_prior\n",
    "    def log_likelihood_prior_probability(self, X):\n",
    "        return X @ (np.log(self.W).T) + (1 - X) @ (np.log(1-self.W).T) + np.log(self.W_prior)\n",
    "\n",
    "model = NaiveBayesBernoulli()       \n",
    "log_likelihood_prior = model.log_likelihood_prior_probability(X1)\n",
    "print(\"--------------Binary Classification----------\")\n",
    "print(f\"log likelihood of Prior probabilities of class : {log_likelihood_prior}\")\n",
    "\n",
    "print(\"-------------multi-class classification-----------\")\n",
    "log_likelihood_prior = model.log_likelihood_prior_probability(X2)\n",
    "print(f\"log likelihood of prior probabilities of class : {log_likelihood_prior}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b73847",
   "metadata": {},
   "source": [
    "Using Bayes’ theorem, the **posterior probability** of class (c) is:\n",
    "\n",
    "$$\n",
    "P(y=c \\mid x) = \\frac{P(x \\mid y=c)\\pi_c}{\\sum_{c'=1}^K P(x \\mid y=c')\\pi_{c'}} = \\frac{P(x \\mid y=c) P(y = c)}{\\sum_{c'=1}^K P(x \\mid y=c')P(y=c')}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f6b338a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveBayesBernoulli(object):\n",
    "    def __init__(self, alpha=0.1):\n",
    "        self.alpha = alpha\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Task : Estimation of parameters of bernoulli distribution by naive bayes\n",
    "        Inputs :\n",
    "            X: Feature matrix of shape (n,m)\n",
    "            y: Label vector of shape (n,) \n",
    "        Outputs:\n",
    "            w_{j y_c}, w_prior\n",
    "        \"\"\"\n",
    "        n_samples, m_features = X.shape\n",
    "        n_classes = len(np.unique(y))\n",
    "\n",
    "        # Inialization of weight matrix\n",
    "        self.W = np.zeros((n_classes,m_features), dtype=np.float64)\n",
    "        self.W_prior = np.zeros(n_classes, dtype=np.float64)\n",
    "\n",
    "        # Processing samples for each classes seperatly\n",
    "        for c in range(n_classes):\n",
    "            # 1. find example with label c\n",
    "            X_c = X[y==c]\n",
    "            # 2. Estimation of W{j, y_c} = P(x_j | y_c = c)\n",
    "            self.W[c, :] = (np.sum(X_c, axis=0) + self.alpha)/(X_c.shape[0] + 2.0 * self.alpha)\n",
    "            # 3. Estimation of prior\n",
    "            self.W_priors[c] = (X_c.shape[0] + self.alpha)/(float(n_samples) + n_classes * self.alpha)\n",
    "        \n",
    "        return self.W, self.W_prior\n",
    "    def log_likelihood_prior_probability(self, X):\n",
    "        return X @ (np.log(self.W).T) + (1 - X) @ (np.log(1-self.W).T) + np.log(self.W_prior)\n",
    "\n",
    "        \n",
    "    def posterior_probability(self, X):\n",
    "        q = self.log_likelihood_prior_probability(X)\n",
    "        return np.exp(q) / np.expand_dims(np.sum(np.exp(q), axis=1), axis = 1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db84a184",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NaiveBayesBernoulli()       \n",
    "log_likelihood_prior = model.posterior_probability(X1)\n",
    "print(\"--------------Binary Classification----------\")\n",
    "print(f\"log likelihood of Prior probabilities of class : {log_likelihood_prior}\")\n",
    "\n",
    "print(\"-------------multi-class classification-----------\")\n",
    "log_likelihood_prior = model.log_likelihood_prior_probability(X2)\n",
    "print(f\"log likelihood of prior probabilities of class : {log_likelihood_prior}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35003a78",
   "metadata": {},
   "source": [
    "Finally, we predict the **most probable class** using the **maximum a posteriori (MAP) estimate**:\n",
    "\n",
    "$$\n",
    "\\hat{y} = \\arg\\max_c P(y=c \\mid x)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c066b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveBayesBernoulli(object):\n",
    "    def __init__(self, alpha=0.1):\n",
    "        self.alpha = alpha\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Task : Estimation of parameters of bernoulli distribution by naive bayes\n",
    "        Inputs :\n",
    "            X: Feature matrix of shape (n,m)\n",
    "            y: Label vector of shape (n,) \n",
    "        Outputs:\n",
    "            w_{j y_c}, w_prior\n",
    "        \"\"\"\n",
    "        n_samples, m_features = X.shape\n",
    "        n_classes = len(np.unique(y))\n",
    "\n",
    "        # Inialization of weight matrix\n",
    "        self.W = np.zeros((n_classes,m_features), dtype=np.float64)\n",
    "        self.W_prior = np.zeros(n_classes, dtype=np.float64)\n",
    "\n",
    "        # Processing samples for each classes seperatly\n",
    "        for c in range(n_classes):\n",
    "            # 1. find example with label c\n",
    "            X_c = X[y==c]\n",
    "            # 2. Estimation of W{j, y_c} = P(x_j | y_c = c)\n",
    "            self.W[c, :] = (np.sum(X_c, axis=0) + self.alpha)/(X_c.shape[0] + 2.0 * self.alpha)\n",
    "            # 3. Estimation of prior\n",
    "            self.W_priors[c] = (X_c.shape[0] + self.alpha)/(float(n_samples) + n_classes * self.alpha)\n",
    "        \n",
    "        return self.W, self.W_prior\n",
    "    def log_likelihood_prior_probability(self, X):\n",
    "        return X @ (np.log(self.W).T) + (1 - X) @ (np.log(1-self.W).T) + np.log(self.W_prior)\n",
    "\n",
    "        \n",
    "    def posterior_probability(self, X):\n",
    "        q = self.log_likelihood_prior_probability(X)\n",
    "        return np.exp(q) / np.expand_dims(np.sum(np.exp(q), axis=1), axis = 1)\n",
    "      \n",
    "    def predict(self, X):\n",
    "        return np.argmax(self.log_likelihood_prior_probability(X), axis=1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f42f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NaiveBayesBernoulli()       \n",
    "log_likelihood_prior = model.posterior_probability(X1)\n",
    "print(\"--------------Binary Classification----------\")\n",
    "print(f\"log likelihood of Prior probabilities of class : {log_likelihood_prior}\")\n",
    "\n",
    "print(\"-------------multi-class classification-----------\")\n",
    "log_likelihood_prior = model.log_likelihood_prior_probability(X2)\n",
    "print(f\"log likelihood of prior probabilities of class : {log_likelihood_prior}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "642b6cd9",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
