{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d0fa7d4",
   "metadata": {},
   "source": [
    "### Mathematics of the Random Forest Algorithm\n",
    "I will explore **mathematical foundation of the Random Forest algorithm**, which is an **ensemble of decision trees** trained using **bagging (bootstrap aggregation)** and **random feature selection**. Let’s go step by step through its **mathematical formulation**, both for **regression** and **classification**, linking to the idea of **variance reduction** and **ensemble averaging**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c74ceba8",
   "metadata": {},
   "source": [
    "**Given** a dataset\n",
    "$$\n",
    "D = {(x_i, y_i)}_{i=1}^n, \\quad x_i \\in \\mathbb{R}^m, \\quad y_i \\in \\mathcal{Y}\n",
    "$$\n",
    "where\n",
    "$$\n",
    "\\mathcal{Y} =\n",
    "\\begin{cases}\n",
    "\\mathbb{R}, & \\text{for regression} [4pt]\n",
    "{1, 2, \\dots, K}, & \\text{for classification}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "We aim to learn a predictor $h(x)$ that generalizes well by **aggregating multiple decision trees** $h_j(x)$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "109fb96b",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "05488c72",
   "metadata": {},
   "source": [
    "###  **Bootstrap Sampling**\n",
    "\n",
    "We draw $q$ bootstrap samples, each of size $n$, **with replacement**:\n",
    "\n",
    "$$\n",
    "D_j = {(x_i^{(j)}, y_i^{(j)})}_{i=1}^{n}, \\quad j = 1, 2, \\dots, q\n",
    "$$\n",
    "\n",
    "Each $D_j$ is drawn from the original dataset $D$, such that approximately **63.2% of the original samples** appear in each bootstrap sample (the rest are “out-of-bag” samples)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd51b88",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b141577e",
   "metadata": {},
   "source": [
    "### **Random Feature Subsampling**\n",
    "\n",
    "At **each split** within the decision tree $h_j(x)$:\n",
    "\n",
    "* Randomly select a subset of features\n",
    "  $$\n",
    "  \\mathcal{F}_j \\subseteq {1, 2, \\dots, m}, \\quad |\\mathcal{F}_j| = u \\le m\n",
    "  $$\n",
    "* Choose the **best feature $f^* \\in \\mathcal{F}_j$** and **split point** based on a **split criterion** (e.g., Gini, entropy, or MSE).\n",
    "\n",
    "This introduces **decorrelation** between trees, reducing the overall **variance** of the ensemble.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3203910",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6a696d63",
   "metadata": {},
   "source": [
    "### **Individual Tree Learning**\n",
    "\n",
    "Each tree $h_j(x)$ is trained independently on $D_j$ using the selected feature subsets.\n",
    "\n",
    "Each $h_j(x)$ approximates the **true function** $f(x)$ as:\n",
    "$$\n",
    "h_j(x) = f(x) + \\epsilon_j(x)\n",
    "$$\n",
    "where $\\epsilon_j(x)$ is the model-specific error (random due to sampling and feature randomness)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a296de5",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5d4d2d33",
   "metadata": {},
   "source": [
    "### **Ensemble Aggregation**\n",
    "\n",
    "After training $q$ trees, Random Forest aggregates them into a single predictor $h(x)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f7f5d0e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cabbb44c",
   "metadata": {},
   "source": [
    "### **For Regression: Averaging**\n",
    "\n",
    "$$\n",
    "\\boxed{\n",
    "h(x) = \\frac{1}{q} \\sum_{j=1}^q h_j(x)\n",
    "}\n",
    "$$\n",
    "\n",
    "* The final prediction is the **mean** of all individual tree predictions.\n",
    "* This reduces the variance:\n",
    "$Var h(x) = \\rho \\sigma^2 + \\frac{1 - \\rho}{q}\\sigma^2$, where $\\rho$ is the average pairwise correlation between trees.\n",
    "\n",
    "Hence, by reducing correlation (via random feature selection), the ensemble **reduces variance**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5764335b",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "adcbf95f",
   "metadata": {},
   "source": [
    "### **For Classification: Majority Voting**\n",
    "\n",
    "Each tree outputs a class prediction:\n",
    "$$\n",
    "h_j(x) \\in {1, 2, \\dots, K}\n",
    "$$\n",
    "Then the final classifier is:\n",
    "$$\n",
    "\\boxed{\n",
    "h(x) = \\arg\\max_{k \\in \\mathcal{Y}} \\sum_{j=1}^q \\mathbf{1}\\big(h_j(x) = k\\big)\n",
    "}\n",
    "$$\n",
    "where $\\mathbf{1}(\\cdot)$ is the indicator function (equals 1 if true, 0 otherwise).\n",
    "\n",
    "This is a **majority voting** rule — the most common class among all trees is selected.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65362c23",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "71a89535",
   "metadata": {},
   "source": [
    "### **Bias–Variance Decomposition**\n",
    "\n",
    "If $h_j(x)$ are unbiased estimators of $f(x)$ with variance $\\sigma^2$ and correlation $\\rho$, the ensemble variance is:\n",
    "\n",
    "$$\n",
    "\\operatorname{Var}[h(x)] = \\rho \\sigma^2 + \\frac{1 - \\rho}{q} \\sigma^2\n",
    "$$\n",
    "\n",
    "This means:\n",
    "\n",
    "* As $q \\to \\infty$, variance → $\\rho \\sigma^2$.\n",
    "* Lower correlation $\\rho$ ⇒ stronger variance reduction.\n",
    "\n",
    "Hence, **decorrelation (via feature subsampling)** is key to Random Forest’s success."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29557980",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e4141cb1",
   "metadata": {},
   "source": [
    "### **Out-of-Bag (OOB) Error Estimate**\n",
    "\n",
    "Each sample $x_i$ is not used in approximately 1/3 of trees.\n",
    "We can estimate the model’s generalization error as:\n",
    "\n",
    "$$\n",
    "\\text{OOB Error} = \\frac{1}{n} \\sum_{i=1}^n \\mathbf{1}\\big(y_i \\ne \\hat{y}_i^{OOB}\\big)\n",
    "$$\n",
    "where $\\hat{y}_i^{OOB}$ is the majority vote among trees that did *not* include $(x_i, y_i)$ during training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b0abde",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b5ea01a1",
   "metadata": {},
   "source": [
    "### **Summary Table**\n",
    "\n",
    "| Step | Mathematical Operation                                            | Purpose                    |\n",
    "| ---- | ----------------------------------------------------------------- | -------------------------- |\n",
    "| 1    | $D_j \\sim D$ (bootstrap with replacement)                       | Data randomness            |\n",
    "| 2    | Randomly select $u \\le m$ features per split                    | Feature randomness         |\n",
    "| 3    | Train each tree $h_j(x)$ independently                          | Base learner               |\n",
    "| 4a   | $h(x) = \\frac{1}{q}\\sum h_j(x)$                                 | Regression aggregation     |\n",
    "| 4b   | $h(x) = \\arg\\max_k \\sum 1(h_j(x)=k)$                            | Classification aggregation |\n",
    "| 5    | $\\text{OOB Error} = \\frac{1}{n}\\sum 1(y_i \\ne \\hat{y}_i^{OOB})$ | Model evaluation           |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbaff2f6",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
