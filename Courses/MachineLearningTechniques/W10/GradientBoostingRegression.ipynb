{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd5f1096",
   "metadata": {},
   "source": [
    "# W10 : Gradient Boosting for Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b5ba90",
   "metadata": {},
   "source": [
    "### **Problem Setup**\n",
    "\n",
    "We have a dataset:\n",
    "$$\n",
    "D = {(x_i, y_i)}_{i=1}^n, \\quad x_i \\in \\mathbb{R}^d, , y_i \\in \\mathbb{R}\n",
    "$$\n",
    "Our goal is to learn a function $F(x)$ that minimizes an empirical risk (loss function):\n",
    "\n",
    "$$\n",
    "\\min_{F} ; \\mathcal{L} = \\sum_{i=1}^{n} L(y_i, F(x_i))\n",
    "$$\n",
    "\n",
    "where $L(y, F(x))$ is typically a convex differentiable loss function (e.g., Mean Squared Error).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee37cc1",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ede60e7f",
   "metadata": {},
   "source": [
    "### **Step 1: Initialization**\n",
    "\n",
    "We start with an initial model $F_0(x)$ that minimizes the loss function over a constant prediction:\n",
    "\n",
    "$$\n",
    "F_0(x) = \\arg\\min_{c} \\sum_{i=1}^n L(y_i, c)\n",
    "$$\n",
    "\n",
    "For **squared loss** $L(y, F(x)) = \\frac{1}{2}(y - F(x))^2$,\n",
    "this simplifies to the mean of the targets:\n",
    "$$\n",
    "F_0(x) = \\bar{y} = \\frac{1}{n}\\sum_{i=1}^{n} y_i\n",
    "$$\n",
    "which matches your step:\n",
    "$$\n",
    "y_{\\text{train}*{p_0}} = \\frac{1}{n}\\sum*{i=1}^{n} y_{\\text{train}*i}, \\quad y*{\\text{test}*{p_0}} = y*{\\text{train}_{p_0}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f378f7",
   "metadata": {},
   "source": [
    "### **Step 2: Compute Residuals (Pseudo-Residuals)**\n",
    "\n",
    "At iteration $m$, compute the **negative gradient** of the loss function with respect to predictions $F_{m-1}(x_i)$ and\n",
    "$$r_{im} = -\\left[\\frac{\\partial L(y_i, F(x_i))}{\\partial F(x_i)}\\right]*{F(x_i)=F*{m-1}(x_i)}$$\n",
    "\n",
    "For **squared loss**, this becomes:\n",
    "$$r_{im} = y_i - F_{m-1}(x_i)$$\n",
    "This is exactly your residual formula:\n",
    "$$\n",
    "r_0 = y_{\\text{train}} - y_{\\text{train}_{p_0}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f3b8ff2",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9fd7d8ed",
   "metadata": {},
   "source": [
    "### **Step 3: Fit a Weak Learner**\n",
    "\n",
    "Train a weak learner (e.g., a regression tree) $h_m(x)$ to predict the residuals $r_{im}$:\n",
    "\n",
    "$$\n",
    "h_m(x) \\approx r_{im}\n",
    "$$\n",
    "\n",
    "That means the learner fits to the direction of the **steepest descent** in function space (the negative gradient).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30bcdda7",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c785f5d0",
   "metadata": {},
   "source": [
    "### **Step 4: Compute Step Size (Learning Rate)**\n",
    "\n",
    "Optionally, compute the optimal multiplier $\\gamma_m$ for the weak learner to minimize the overall loss:\n",
    "\n",
    "$$\n",
    "\\gamma_m = \\arg\\min_{\\gamma} \\sum_{i=1}^{n} L(y_i, F_{m-1}(x_i) + \\gamma h_m(x_i))\n",
    "$$\n",
    "\n",
    "For squared loss:\n",
    "$$\n",
    "\\gamma_m = 1\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10add0df",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a849fb5d",
   "metadata": {},
   "source": [
    "### **Step 5: Update the Model**\n",
    "\n",
    "Update the model by adding the scaled weak learner to the ensemble:\n",
    "\n",
    "$$\n",
    "F_m(x) = F_{m-1}(x) + \\alpha \\gamma_m h_m(x)\n",
    "$$\n",
    "\n",
    "where $\\alpha \\in (0,1)$ is the **learning rate** that controls the contribution of each weak learner.\n",
    "\n",
    "Your equivalent formula\n",
    "$$\n",
    "y_{\\text{train}*{p_1}} = y*{\\text{train}*{p_0}} + \\alpha f_0(X*{\\text{train}})\n",
    "$$\n",
    "$$\n",
    "y_{\\text{test}*{p_1}} = y*{\\text{test}*{p_0}} + \\alpha f_0(X*{\\text{test}})\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e56255f3",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0432a2a7",
   "metadata": {},
   "source": [
    "### **Step 6: Repeat**\n",
    "\n",
    "Repeat steps 2â€“5 for $M$ boosting rounds, giving the final model:\n",
    "\n",
    "$$\n",
    "F_M(x) = F_0(x) + \\alpha \\sum_{m=1}^{M} \\gamma_m h_m(x)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f886021d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7fe02c39",
   "metadata": {},
   "source": [
    "### **For Squared Loss (Regression)**\n",
    "\n",
    "If $L(y, F(x)) = \\frac{1}{2}(y - F(x))^2$, then:\n",
    "\n",
    "* $r_{im} = y_i - F_{m-1}(x_i)$\n",
    "* $\\gamma_m = 1$\n",
    "* Final model:\n",
    "  $$\n",
    "  F_M(x) = \\bar{y} + \\alpha \\sum_{m=1}^{M} h_m(x)\n",
    "  $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65cf2f4a",
   "metadata": {},
   "source": [
    "### **Summary Table**\n",
    "\n",
    "| Step | Mathematical Operation                                                   | Description                       |\n",
    "| ---- | ------------------------------------------------------------------------ | --------------------------------- |\n",
    "| 1    | $F_0(x) = \\arg\\min_c \\sum L(y_i, c)$                                   | Initialize model with constant    |\n",
    "| 2    | $r_{im} = -\\frac{\\partial L}{\\partial F(x_i)}$                         | Compute pseudo-residuals          |\n",
    "| 3    | $h_m(x) \\approx r_{im}$                                                | Fit weak learner                  |\n",
    "| 4    | $\\gamma_m = \\arg\\min_\\gamma \\sum L(y_i, F_{m-1}(x_i)+\\gamma h_m(x_i))$ | Compute step size                 |\n",
    "| 5    | $F_m(x) = F_{m-1}(x) + \\alpha \\gamma_m h_m(x)$                         | Update model                      |\n",
    "| 6    | Repeat                                                                   | Until convergence or $M$ rounds |"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
