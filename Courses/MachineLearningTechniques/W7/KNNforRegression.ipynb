{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "94f78bba",
   "metadata": {},
   "source": [
    "# W7 : K-Nearest Neighbours(KNN) for Regression\n",
    "KNN is a non-parameteric and instance based learning algorithm unlike parameteric models like Naive Bays does't assume any distribution on the data instead it predicts based on similarity between examples.\n",
    "\n",
    "It estimates the target value for a new input $x_{\\text{new}}$ as the **average of its nearest neighbors** in the feature space."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1758430e",
   "metadata": {},
   "source": [
    "### Problem Setup\n",
    "$$\n",
    "\\mathcal{D} = {(x_i, y_i)}_{i=1}^n, \\quad x_i \\in \\mathbb{R}^m, ; y_i \\in \\mathbb{R}\n",
    "$$\n",
    "\n",
    "For a new input $x_{\\text{new}}$, we wish to estimate:\n",
    "\n",
    "$$\n",
    "\\hat{y}(x_{\\text{new}}) \\approx f(x_{\\text{new}}) = \\mathbb{E} [Y | X] = x_{\\text{new}}$$\n",
    "\n",
    "However, since we do not assume any parametric form of ( f ), we approximate it **empirically** using local averaging.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4facc4d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3abf6664",
   "metadata": {},
   "source": [
    "### Distance Metric\n",
    "\n",
    "To find neighbors, we compute distances between the new example and all training examples\n",
    "\n",
    "$$\n",
    "d(x_i, x_{\\text{new}}) = \\sqrt{\\sum_{j=1}^m (x_{ij} - x_{\\text{new},j})^2}\n",
    "$$\n",
    "\n",
    "Then, we sort all points by increasing distance and select the indices of the (k) nearest ones\n",
    "\n",
    "$$\n",
    "\\text{KNN}(x_{\\text{new}}) = {i_1, i_2, \\dots, i_k} \\text{ where } d(x_{i_1}, x_{\\text{new}}) \\le \\dots \\le d(x_{i_k}, x_{\\text{new}})\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17096763",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "02d1ee0d",
   "metadata": {},
   "source": [
    "### Prediction Rule (Local Averaging)\n",
    "\n",
    "The predicted value is the **mean of target values** of these (k) nearest neighbors:\n",
    "\n",
    "$$\n",
    "\\hat{y}(x_{\\text{new}}) = \\frac{1}{k} \\sum_{i \\in \\text{KNN}(x_{\\text{new}})} y_i\n",
    "$$\n",
    "\n",
    "This can also be seen as a **weighted average** if we use distance-based weights:\n",
    "\n",
    "$$\n",
    "\\hat{y}(x_{\\text{new}}) = \\frac{\\sum_{i=1}^n w_i(x_{\\text{new}}) y_i}{\\sum_{i=1}^n w_i(x_{\\text{new}})}, \\quad\n",
    "w_i(x_{\\text{new}}) =\n",
    "\\begin{cases}\n",
    "1, & \\text{if } i \\in \\text{KNN}(x_{\\text{new}}) \\\n",
    "0, & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "> Weighted KNN can use $w_i = \\frac{1}{d(x_i, x_{\\text{new}}) + \\epsilon}$ to give closer points more importance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "303300c0",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0d09824f",
   "metadata": {},
   "source": [
    "### Bias–Variance Tradeoff\n",
    "* Small (k): low bias, high variance → overfits to local noise\n",
    "* Large (k): high bias, low variance → overly smooths the curve\n",
    "\n",
    "Thus, (k) controls the **locality** of the regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "857ec9a9",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0afb51af",
   "metadata": {},
   "source": [
    "### Evaluation Metric — Mean Squared Error (MSE)\n",
    "\n",
    "The **loss function** for regression is typically Mean Squared Error (MSE):\n",
    "\n",
    "$$\n",
    "\\text{MSE} = \\frac{1}{n_{\\text{test}}} \\sum_{i=1}^{n_{\\text{test}}} (y_i - \\hat{y}_i)^2\n",
    "$$\n",
    "\n",
    "or equivalently, the **Root Mean Squared Error (RMSE)**\n",
    "\n",
    "$$\n",
    "\\text{RMSE} = \\sqrt{\\frac{1}{n_{\\text{test}}} \\sum_{i=1}^{n_{\\text{test}}} (y_i - \\hat{y}_i)^2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "520049d3",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d73ac7f0",
   "metadata": {},
   "source": [
    "### Summary\n",
    "| Concept               | Mathematical Expression                                                  | Python Code Equivalent                                                    |\n",
    "| --------------------- | ------------------------------------------------------------------------ | ------------------------------------------------------------------------- |\n",
    "| Distance              | $d(x_i, x_{\\text{new}}) = \\sqrt{\\sum_j (x_{ij} - x_{\\text{new},j})^2}$ | `distance_vector = self._distance_metric(self._X, new_example)`           |\n",
    "| Nearest Neighbors     | $\\text{KNN}(x_{\\text{new}})$ = k smallest distances                      | `knn_indices = np.argpartition(distance_vector, self._k)[:self._k]`       |\n",
    "| Regression Prediction | $\\hat{y} = \\frac{1}{k}\\sum_{i\\in KNN} y_i$                               | `label = knn.mean()`                                                      |\n",
    "| MSE Loss              | $\\sum (y - \\hat{y})^2$                                                   | `np.sum(np.power(y_test - y_test_predicted, 2))`                          |\n",
    "| RMSE                  | $\\sqrt{\\frac{1}{n}\\sum (y - \\hat{y})^2}$                                 | `np.sqrt((error_vector.T @ error_vector)/ error_vector.ravel().shape[0])` |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "128c6690",
   "metadata": {},
   "source": [
    "how increasing (k) affects model smoothness:\n",
    "\n",
    "* (k=1): exact local fit (overfitting)\n",
    "* (k=16): smooth average (underfitting)\n",
    "Thus, How to identify the (k) that minimizes prediction error — giving the **best bias-variance tradeoff**.  Here is the $E(k) = \\sum_i (y_i - \\hat{y}_i(k))^2$ error agaist k.\n",
    "\n",
    "| Aspect          | Description                    |\n",
    "| --------------- | ------------------------------ |\n",
    "| Model Type      | Non-parametric, Instance-based |\n",
    "| Assumption      | Locally smooth function (f(x)) |\n",
    "| Prediction      | Mean of k nearest neighbors    |\n",
    "| Hyperparameter  | Number of neighbors (k)        |\n",
    "| Distance Metric | Euclidean (commonly)           |\n",
    "| Loss            | MSE or RMSE                    |\n",
    "| Bias-Variance   | Controlled by (k)              |"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
