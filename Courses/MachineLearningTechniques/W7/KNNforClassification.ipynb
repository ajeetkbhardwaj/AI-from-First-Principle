{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea633687",
   "metadata": {},
   "source": [
    "# W7 : K-Nearest Neighbours(KNN) for Classification\n",
    "KNN is a non-parameteric and instance based learning algorithm unlike parameteric models like Naive Bays does't assume any distribution on the data instead it predicts based on similarity between examples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42e9e232",
   "metadata": {},
   "source": [
    "### Problem Setup\n",
    "Let the training dataset be\n",
    "$$\n",
    "\\mathcal{D} = {(x_i, y_i)}_{i=1}^n, \\quad x_i \\in \\mathbb{R}^m, \\ y_i \\in \\{1, 2, \\dots, K\\}\n",
    "$$\n",
    "Our aim for given a **new example** $x_{new}$, and we want to predict its class label $\\hat{y}$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee75e217",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e757a9f9",
   "metadata": {},
   "source": [
    "### Distance Metric\n",
    "\n",
    "KNN uses a **distance metric** to measure similarity between the test sample $x_{new}$ and each training sample $x_i$.\n",
    "The most common choice is **Euclidean distance**\n",
    "$$\n",
    "d(x_i, x_{new}) = \\sqrt{\\sum_{j=1}^m (x_{ij} - x_{new,j})^2}\n",
    "$$\n",
    "\n",
    "It computes distances between all training samples and the new example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b43f712",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ad5aae81",
   "metadata": {},
   "source": [
    "### K-Nearest Neighbors(KNN)\n",
    "After computing distances, we select the **k closest samples** (the nearest neighbors):\n",
    "\n",
    "$$\n",
    "\\text{KNN}(x_{new}) = { i_1, i_2, \\dots, i_k } \\quad \\text{such that} \\quad d(x_{i_1}, x_{new}) \\le d(x_{i_2}, x_{new}) \\le \\dots\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc2a9426",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "24e7be71",
   "metadata": {},
   "source": [
    "### Majority Voting (Classification)\n",
    "\n",
    "For **classification**, the predicted label is the **most frequent** class among the (k) neighbors:\n",
    "\n",
    "$$\n",
    "\\hat{y} = \\underset{c \\in \\mathcal{C}}{\\arg\\max} \\sum_{i \\in \\text{KNN}(x_{new})} \\mathbb{I}(y_i = c)\n",
    "$$\n",
    "\n",
    "where $\\mathbb{I}$ is the indicator function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec240e9a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "49cd27ab",
   "metadata": {},
   "source": [
    "### Evaluation Metric\n",
    "\n",
    "* **Classification:**\n",
    "  Misclassification rate (accuracy complement):\n",
    "\n",
    "$$\n",
    "\\text{Accuracy} = \\frac{1}{n_{test}} \\sum_{i=1}^{n_{test}} \\mathbb{I}(\\hat{y}_i = y_i)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90832513",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ef71fedb",
   "metadata": {},
   "source": [
    "### Summary Table\n",
    "\n",
    "| Concept                 | Formula                                                                 | Code                        |\n",
    "| ----------------------- | ----------------------------------------------------------------------- | --------------------------- |\n",
    "| Distance                | $d(x_i, x_{new}) = \\sqrt{\\sum_j (x_{ij} - x_{new,j})^2}$                | `distance_metric`           |\n",
    "| K Nearest Neighbors     | $\\text{KNN}(x_{new}) = { i_1, \\dots, i_k}$                              | `np.argpartition()`         |\n",
    "| Classification Rule     | $\\hat{y} = \\arg\\max_c \\sum_{i \\in KNN} \\mathbb{I}(y_i = c)$             | `stats.mode(knn)[0]`        |\n",
    "| Regression Rule         | $\\hat{y} = \\frac{1}{k} \\sum_{i \\in KNN} y_i$                            | `knn.mean()`                |\n",
    "| Classification Accuracy | $\\text{Accuracy} = \\frac{1}{n_{test}} \\sum \\mathbb{I}(\\hat{y}_i = y_i)$ | `np.mean(y_test == y_pred)` |\n",
    "| RMSE (Regression)       | $\\sqrt{\\frac{1}{n_{test}} \\sum (\\hat{y}_i - y_i)^2}$                   | `error_vector`              |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1063e06",
   "metadata": {},
   "source": [
    "* **Choice of (k):**\n",
    "  * Small (k): model becomes sensitive to noise (high variance).\n",
    "  * Large (k): model becomes too smooth (high bias).\n",
    "\n",
    "* **Normalization:** Always scale features before applying KNN, as distance is sensitive to feature scales.\n",
    "\n",
    "* **Complexity:**\n",
    "  Prediction is (O(n \\times m)) per test point (can be optimized via KD-trees or Ball trees)."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
