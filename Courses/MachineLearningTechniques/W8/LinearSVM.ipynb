{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61b53ecf",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">Support Vector Machine</h1>\n",
    "\n",
    "## Table of Contents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f12e03",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "13440b45",
   "metadata": {},
   "source": [
    "### 1. Linear SVM Algorithm\n",
    "Assume that we have a linearly seperable dataset. Then We needed to find a hyperplane that seperates two classes with maximum margin.\n",
    "\n",
    "Consider the datasets for binary classification task\n",
    "$$\n",
    "D = \\{(x_i, y_i) : y_i \\in \\{-1, +1\\}, \\quad x_i \\in \\mathbb{R}^n \\}\n",
    "$$\n",
    "and we want to find a hyperplane $W^T X + b = 0$, where $W$ be weight vector normal to hyperplane and $b$ be bias as offset.\n",
    "\n",
    "Lets consider the decision model of the form\n",
    "$$\n",
    "y = sign(X W + b)\n",
    "$$\n",
    "Distance of a point (x_i) from the hyperplane\n",
    "  $\n",
    "  \\frac{|w^T x_i + b|}{|w|}\n",
    "  $ want **maximum margin**, i.e. maximize $\\frac{2}{|w|}$ subject to correct classification.\n",
    "\n",
    "Hard Margin(Linearly Seperable) SVM \n",
    "Optimization Problem :\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\text{Minimize: } & \\frac{1}{2} |w|^2 \\\n",
    "\\text{Subject to: } & y_i (w^T x_i + b) \\geq 1, \\quad \\forall i\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "* The **1/2** term simplifies differentiation.\n",
    "* $y_i (w^T x_i + b) \\geq 1$ means all points are correctly classified and outside margin boundaries.\n",
    "\n",
    "\n",
    "Lagrange Multipliers Method for Dual\n",
    "$$\n",
    "L(w,b,\\alpha) = \\frac{1}{2}|w|^2 - \\sum_{i=1}^{m}\\alpha_i y_i(w^T x_i + b) - 1$$\n",
    "\n",
    "where $(\\alpha_i \\geq 0)$ are **Lagrange multipliers**.\n",
    "\n",
    "or \n",
    "\n",
    "$$\n",
    "J(W, b) = \\frac{1}{2} ||W||^2 + C \\sum_i  max\\left(0, 1 - y^i(W^T X^i + b)\\right)\n",
    "$$\n",
    "\n",
    "Take derivatives and set to zero\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial w} = 0 \\Rightarrow w = \\sum_{i=1}^{m}\\alpha_i y_i x_i\n",
    "$$\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial b} = 0 \\Rightarrow \\sum_{i=1}^{m}\\alpha_i y_i = 0\n",
    "$$\n",
    "\n",
    "Substitute back into (L):\n",
    "$$\n",
    "\\text{Dual Problem: }\n",
    "\\begin{aligned}\n",
    "\\max_{\\alpha} & \\sum_{i=1}^{m}\\alpha_i - \\frac{1}{2}\\sum_{i,j=1}^{m}\\alpha_i \\alpha_j y_i y_j x_i^T x_j \\\n",
    "\\text{s.t. } & \\alpha_i \\geq 0, \\ \\sum_i \\alpha_i y_i = 0\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "After solving for (\\alpha_i), compute:\n",
    "$$\n",
    "w = \\sum_i \\alpha_i y_i x_i, \\quad b = y_k - w^T x_k \\text{ (for any support vector } k)\n",
    "$$\n",
    "\n",
    "Only points with (\\alpha_i > 0) are **support vectors**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63edc3c0",
   "metadata": {},
   "source": [
    "### **1.3 Soft Margin SVM (Non-separable Case)**\n",
    "When data overlaps, we introduce **slack variables** (\\xi_i).\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\text{Minimize: } & \\frac{1}{2}|w|^2 + C \\sum_{i=1}^{m}\\xi_i \\\n",
    "\\text{Subject to: } & y_i (w^T x_i + b) \\ge 1 - \\xi_i, \\quad \\xi_i \\ge 0\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "* (C) controls trade-off between **margin size** and **classification error**:\n",
    "\n",
    "  * Large (C): low bias, high variance (strict classification)\n",
    "  * Small (C): high bias, low variance (more tolerance for error)\n",
    "\n",
    "Dual form (similar to before but with bounded multipliers):\n",
    "$$\n",
    "\\max_{\\alpha} \\sum_i \\alpha_i - \\frac{1}{2}\\sum_{i,j} \\alpha_i \\alpha_j y_i y_j x_i^T x_j\n",
    "$$\n",
    "$$\n",
    "\\text{s.t. } 0 \\le \\alpha_i \\le C, \\ \\sum_i \\alpha_i y_i = 0\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf309d8",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3a30eb6a",
   "metadata": {},
   "source": [
    "## ðŸ”® 2. Nonlinear SVM (Kernel Trick)\n",
    "\n",
    "When data is not linearly separable, we **map** it to a higher-dimensional space:\n",
    "$$\n",
    "\\phi: \\mathbb{R}^n \\rightarrow \\mathbb{R}^p\n",
    "$$\n",
    "and use:\n",
    "$$\n",
    "K(x_i, x_j) = \\phi(x_i)^T \\phi(x_j)\n",
    "$$\n",
    "without ever computing (\\phi) explicitly.\n",
    "\n",
    "### **Dual Problem with Kernel**\n",
    "\n",
    "$$\n",
    "\\max_{\\alpha} \\sum_i \\alpha_i - \\frac{1}{2}\\sum_{i,j} \\alpha_i \\alpha_j y_i y_j K(x_i, x_j)\n",
    "$$\n",
    "$$\n",
    "\\text{s.t. } 0 \\le \\alpha_i \\le C, \\ \\sum_i \\alpha_i y_i = 0\n",
    "$$\n",
    "\n",
    "### **Decision Function**\n",
    "\n",
    "$$\n",
    "f(x) = \\text{sign}\\left(\\sum_i \\alpha_i y_i K(x_i, x) + b\\right)\n",
    "$$\n",
    "\n",
    "### **Common Kernels**\n",
    "\n",
    "| Kernel         | Formula                                                | Intuition                                              |\n",
    "| -------------- | ------------------------------------------------------ | ------------------------------------------------------ |\n",
    "| Linear         | (K(x_i, x_j) = x_i^T x_j)                              | No mapping                                             |\n",
    "| Polynomial     | (K(x_i, x_j) = (x_i^T x_j + c)^d)                      | Nonlinear features via degree (d)                      |\n",
    "| RBF (Gaussian) | (K(x_i, x_j) = \\exp(-\\frac{|x_i - x_j|^2}{2\\sigma^2})) | Infinite-dimensional mapping; smooth decision boundary |\n",
    "| Sigmoid        | (K(x_i, x_j) = \\tanh(\\kappa x_i^T x_j + \\theta))       | Similar to neural networks                             |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a73a3e2",
   "metadata": {},
   "source": [
    "Kernel SVM Algorithm - \n",
    "$$\n",
    "\\begin{align}\n",
    "L_d(\\alpha) &= \\sum_{i=1}^n \\alpha^{(i)} - \\frac{1}{2} \\sum_{i=1}^n \\sum_{k=1}^n \\alpha^{(i)} \\alpha^{(k)} y^{(i)} y^{(k)} x^{(i)^T} x^{(k)} \\\\\n",
    "\n",
    "            &= \\sum_{i=1}^n \\alpha^{(i)} - \\frac{1}{2} \\sum_{i=1}^n \\sum_{k=1}^n \\langle \\alpha^{(i)} y^{(i)} x^{(i)} , \\alpha^{(k)}  y^{(k)}  x^{(k)} \\rangle\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17b2e2ff",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
