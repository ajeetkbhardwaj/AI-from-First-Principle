{"cells":[{"cell_type":"markdown","id":"b3d882d2","metadata":{"id":"b3d882d2"},"source":["How to apply transformers in text, image, video, audio and time series etc\n","\n","Transformer model\n","- A backbone architecture to represent and extract information from a sequence of embeddings of discrete tokens.\n","- It takes input of shape (seq_lenght, embedd_size) and outputs tensors of identical shapes\n","- Ex-1 : Images"]},{"cell_type":"code","execution_count":1,"id":"b61a1cf5","metadata":{"id":"b61a1cf5","executionInfo":{"status":"ok","timestamp":1762408236625,"user_tz":-330,"elapsed":9071,"user":{"displayName":"Ajeet Kumar","userId":"04620708898093466225"}}},"outputs":[],"source":["import argparse\n","import math\n","import os\n","from datetime import datetime\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.utils.data import DataLoader\n","from torchvision import transforms\n","from torchvision.datasets import CIFAR10\n","from torchsummary import summary"]},{"cell_type":"code","execution_count":2,"id":"58f57d3d","metadata":{"id":"58f57d3d","executionInfo":{"status":"ok","timestamp":1762408241546,"user_tz":-330,"elapsed":9,"user":{"displayName":"Ajeet Kumar","userId":"04620708898093466225"}}},"outputs":[],"source":["class TransformerEncoderBlock(nn.Module):\n","    \"\"\"\n","\n","    \"\"\"\n","    def __init__(self, embed_dim, ffn_size, num_heads=2, dropout=0.2):\n","        super(TransformerEncoderBlock, self).__init__()\n","        # multi-head attention\n","        self.mhattn = nn.MultiheadAttention(\n","            embed_dim = embed_dim,\n","            num_heads = num_heads,\n","            dropout=dropout,\n","            batch_first=True,\n","        )\n","        # 2. Feed Forward NN\n","        self.ffn = nn.Sequential(\n","            nn.Linear(embed_dim, ffn_size),\n","            nn.GELU(),\n","            nn.Dropout(dropout),\n","            nn.Linear(ffn_size, embed_dim)\n","        )\n","        # 3. Layer normalization\n","        self.input_ln = nn.LayerNorm(embed_dim)\n","        self.ffn_ln = nn.LayerNorm(embed_dim)\n","\n","        # forward pass over the input data x\n","        def forward(self, x):\n","            y = self.input_ln(x)\n","            y, _ = self.mhattn(y, y, y),\n","            y += x\n","\n","            z = self.ffn_ln(y)\n","            z = self.ffn(z)\n","            z += y\n","\n","            return z"]},{"cell_type":"code","execution_count":3,"id":"5228041c","metadata":{"id":"5228041c","executionInfo":{"status":"ok","timestamp":1762408242402,"user_tz":-330,"elapsed":30,"user":{"displayName":"Ajeet Kumar","userId":"04620708898093466225"}}},"outputs":[],"source":["class TransformerEncoderBlock(nn.Module):\n","    \"\"\"Pre-LayerNorm transformer encoder block with MHSA + feedforward network.\"\"\"\n","    def __init__(self, embed_dim, ffn_dim, num_heads=4, dropout=0.1):\n","        super().__init__()\n","        self.ln1 = nn.LayerNorm(embed_dim)\n","        self.mha = nn.MultiheadAttention(embed_dim=embed_dim, num_heads=num_heads, dropout=dropout, batch_first=True)\n","        self.ln2 = nn.LayerNorm(embed_dim)\n","        self.ffn = nn.Sequential(\n","            nn.Linear(embed_dim, ffn_dim),\n","            nn.GELU(),\n","            nn.Dropout(dropout),\n","            nn.Linear(ffn_dim, embed_dim),\n","            nn.Dropout(dropout),\n","        )\n","\n","    def forward(self, x):\n","        # x: (batch, seq_len, embed_dim)\n","        y = self.ln1(x)\n","        attn_out, _ = self.mha(y, y, y, need_weights=False)\n","        x = x + attn_out\n","        z = self.ln2(x)\n","        x = x + self.ffn(z)\n","        return x"]},{"cell_type":"code","execution_count":4,"id":"feeaadcc","metadata":{"id":"feeaadcc","executionInfo":{"status":"ok","timestamp":1762408243145,"user_tz":-330,"elapsed":5,"user":{"displayName":"Ajeet Kumar","userId":"04620708898093466225"}}},"outputs":[],"source":["class VisionTransformer(nn.Module):\n","    \"\"\"\n","\n","    \"\"\"\n","    def __init__(self,\n","                 img_dim,\n","                 num_patches,\n","                 num_classes,\n","                 embed_dim,\n","                 ffn_size,\n","                 num_heads,\n","                 dropout,\n","                 num_blocks):\n","        super(VisionTransformer, self).__init__()"]},{"cell_type":"code","execution_count":5,"id":"de88714e","metadata":{"id":"de88714e","executionInfo":{"status":"ok","timestamp":1762408244328,"user_tz":-330,"elapsed":16,"user":{"displayName":"Ajeet Kumar","userId":"04620708898093466225"}}},"outputs":[],"source":["class VisionTransformer(nn.Module):\n","    def __init__(\n","        self,\n","        image_size=(3, 32, 32),\n","        patch_grid=(8, 8),\n","        embed_dim=64,\n","        ffn_dim=128,\n","        num_heads=4,\n","        num_blocks=4,\n","        num_classes=10,\n","        dropout=0.1,\n","        use_sinusoidal_pos=False,\n","    ):\n","        super().__init__()\n","        C, H, W = image_size\n","        px = H // patch_grid[0]\n","        py = W // patch_grid[1]\n","        assert H % patch_grid[0] == 0 and W % patch_grid[1] == 0, \"Image dims must be divisible by patch grid\"\n","\n","        self.patch_grid = patch_grid\n","        self.patch_size = (px, py)\n","        self.num_patches = patch_grid[0] * patch_grid[1]\n","        self.num_channels = C\n","        self.embed_dim = embed_dim\n","\n","        # linear projection of flattened patch (C * px * py -> embed_dim)\n","        patch_area = C * px * py\n","        self.patch_embed = nn.Linear(patch_area, embed_dim)\n","\n","        # class token\n","        self.class_token = nn.Parameter(torch.randn(1, 1, embed_dim))\n","\n","        # positional encoding (sinusoidal or learnable)\n","        seq_len = self.num_patches + 1\n","        if use_sinusoidal_pos:\n","            self.register_buffer('pos_encoding', self._build_sinusoidal_pos_encoding(seq_len, embed_dim))\n","        else:\n","            self.pos_embedding = nn.Parameter(torch.randn(1, seq_len, embed_dim))\n","\n","        # transformer blocks\n","        self.blocks = nn.ModuleList([\n","            TransformerEncoderBlock(embed_dim=embed_dim, ffn_dim=ffn_dim, num_heads=num_heads, dropout=dropout)\n","            for _ in range(num_blocks)\n","        ])\n","        self.norm = nn.LayerNorm(embed_dim)\n","\n","        # classification head\n","        self.head = nn.Linear(embed_dim, num_classes)\n","\n","    def _build_sinusoidal_pos_encoding(self, seq_len, dim):\n","        pe = torch.zeros(seq_len, dim)\n","        position = torch.arange(0, seq_len, dtype=torch.float).unsqueeze(1)\n","        div_term = torch.exp(torch.arange(0, dim, 2).float() * (-math.log(10000.0) / dim))\n","        pe[:, 0::2] = torch.sin(position * div_term)\n","        pe[:, 1::2] = torch.cos(position * div_term)\n","        return pe.unsqueeze(0)  # (1, seq_len, dim)\n","\n","    def patchify(self, x: torch.Tensor):\n","        # x: (B, C, H, W)\n","        B, C, H, W = x.shape\n","        px, py = self.patch_size\n","        gx, gy = self.patch_grid\n","        # reshape to (B, C, gx, px, gy, py)\n","        x = x.view(B, C, gx, px, gy, py)\n","        # move patch dims next to channels then flatten patch content\n","        # (B, gx, gy, C, px, py)\n","        x = x.permute(0, 2, 4, 1, 3, 5)\n","        # flatten patch spatial & channel dims -> (B, num_patches, patch_area)\n","        x = x.reshape(B, gx * gy, C * px * py)\n","        return x\n","\n","    def forward(self, x: torch.Tensor):\n","        # patchify + embed\n","        patches = self.patchify(x)  # (B, num_patches, patch_area)\n","        patches = self.patch_embed(patches)  # (B, num_patches, embed_dim)\n","\n","        B = patches.size(0)\n","        class_tok = self.class_token.expand(B, -1, -1)  # (B, 1, embed_dim)\n","        x = torch.cat([class_tok, patches], dim=1)  # (B, seq_len, embed_dim)\n","\n","        # add positional encoding\n","        if hasattr(self, 'pos_encoding'):\n","            x = x + self.pos_encoding.to(x.dtype).to(x.device)\n","        else:\n","            x = x + self.pos_embedding\n","\n","        # transformer\n","        for blk in self.blocks:\n","            x = blk(x)\n","        x = self.norm(x)\n","\n","        cls = x[:, 0]  # (B, embed_dim)\n","        logits = self.head(cls)\n","        return logits\n","\n","\n","\n","\n","# ----------------------------\n","# Optional: attention visualization helper (requires model modification)\n","# ----------------------------\n","# Note: The current compact transformer uses nn.MultiheadAttention with need_weights=False in forward to save memory.\n","# To visualize attention, modify TransformerEncoderBlock to return attention weights (set need_weights=True) and collect them.\n","# For brevity we include a simple placeholder function here; users may adjust the block if they need attention maps.\n","\n","\n","\n"]},{"cell_type":"code","execution_count":6,"id":"6355c28f","metadata":{"id":"6355c28f","executionInfo":{"status":"ok","timestamp":1762408248554,"user_tz":-330,"elapsed":27,"user":{"displayName":"Ajeet Kumar","userId":"04620708898093466225"}}},"outputs":[],"source":["# ----------------------------\n","# Training utilities\n","# ----------------------------\n","\n","def get_dataloaders(batch_size=128, data_dir='./data', num_workers=4):\n","    mean = (0.4914, 0.4822, 0.4465)\n","    std = (0.2470, 0.2435, 0.2616)\n","\n","    train_transforms = transforms.Compose([\n","        transforms.RandomCrop(32, padding=4),\n","        transforms.RandomHorizontalFlip(),\n","        transforms.ToTensor(),\n","        transforms.Normalize(mean, std),\n","    ])\n","\n","    test_transforms = transforms.Compose([\n","        transforms.ToTensor(),\n","        transforms.Normalize(mean, std),\n","    ])\n","\n","    train_set = CIFAR10(root=data_dir, train=True, download=True, transform=train_transforms)\n","    test_set = CIFAR10(root=data_dir, train=False, download=True, transform=test_transforms)\n","\n","    train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n","    test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n","    return train_loader, test_loader\n","\n","\n","def train_one_epoch(model, device, dataloader, optimizer, epoch, scaler=None):\n","    model.train()\n","    running_loss = 0.0\n","    correct = 0\n","    total = 0\n","    for X, y in dataloader:\n","        X = X.to(device)\n","        y = y.to(device)\n","        optimizer.zero_grad()\n","        logits = model(X)\n","        loss = F.cross_entropy(logits, y)\n","        loss.backward()\n","        optimizer.step()\n","\n","        running_loss += loss.item() * X.size(0)\n","        preds = logits.argmax(dim=1)\n","        correct += (preds == y).sum().item()\n","        total += X.size(0)\n","\n","    avg_loss = running_loss / total\n","    acc = correct / total\n","    print(f\"Epoch {epoch}: Train loss {avg_loss:.4f}, Train acc {acc:.4f}\")\n","    return avg_loss, acc\n","\n","\n","def evaluate(model, device, dataloader):\n","    model.eval()\n","    running_loss = 0.0\n","    correct = 0\n","    total = 0\n","    with torch.no_grad():\n","        for X, y in dataloader:\n","            X = X.to(device)\n","            y = y.to(device)\n","            logits = model(X)\n","            loss = F.cross_entropy(logits, y, reduction='sum')\n","            running_loss += loss.item()\n","            preds = logits.argmax(dim=1)\n","            correct += (preds == y).sum().item()\n","            total += X.size(0)\n","    avg_loss = running_loss / total\n","    acc = correct / total\n","    print(f\"Eval loss {avg_loss:.4f}, Eval acc {acc:.4f}\")\n","    return avg_loss, acc\n","\n","def save_checkpoint(state, outdir, name=\"best.pth\"):\n","    os.makedirs(outdir, exist_ok=True)\n","    path = os.path.join(outdir, name)\n","    torch.save(state, path)\n","    print(f\"Saved checkpoint: {path}\")\n","\n"]},{"cell_type":"code","execution_count":9,"id":"a606a8ee","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"a606a8ee","executionInfo":{"status":"ok","timestamp":1762408592712,"user_tz":-330,"elapsed":266337,"user":{"displayName":"Ajeet Kumar","userId":"04620708898093466225"}},"outputId":"76aae82b-389e-4d84-cbad-13fe4b16b6ee"},"outputs":[{"output_type":"stream","name":"stdout","text":["Using device: cuda\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 170M/170M [00:14<00:00, 11.9MB/s]\n","/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1: Train loss 1.8406, Train acc 0.3241\n","Eval loss 1.5571, Eval acc 0.4411\n","Saved checkpoint: ./checkpoints/vit_cifar10_best.pth\n","Epoch 2: Train loss 1.5562, Train acc 0.4346\n","Eval loss 1.4083, Eval acc 0.4927\n","Saved checkpoint: ./checkpoints/vit_cifar10_best.pth\n","Epoch 3: Train loss 1.4489, Train acc 0.4748\n","Eval loss 1.3103, Eval acc 0.5267\n","Saved checkpoint: ./checkpoints/vit_cifar10_best.pth\n","Epoch 4: Train loss 1.3731, Train acc 0.5023\n","Eval loss 1.2810, Eval acc 0.5360\n","Saved checkpoint: ./checkpoints/vit_cifar10_best.pth\n","Epoch 5: Train loss 1.3113, Train acc 0.5250\n","Eval loss 1.1954, Eval acc 0.5675\n","Saved checkpoint: ./checkpoints/vit_cifar10_best.pth\n","Epoch 6: Train loss 1.2629, Train acc 0.5427\n","Eval loss 1.1834, Eval acc 0.5721\n","Saved checkpoint: ./checkpoints/vit_cifar10_best.pth\n","Epoch 7: Train loss 1.2220, Train acc 0.5600\n","Eval loss 1.1290, Eval acc 0.5904\n","Saved checkpoint: ./checkpoints/vit_cifar10_best.pth\n","Epoch 8: Train loss 1.1867, Train acc 0.5719\n","Eval loss 1.0914, Eval acc 0.6070\n","Saved checkpoint: ./checkpoints/vit_cifar10_best.pth\n","Epoch 9: Train loss 1.1509, Train acc 0.5856\n","Eval loss 1.0796, Eval acc 0.6094\n","Saved checkpoint: ./checkpoints/vit_cifar10_best.pth\n","Epoch 10: Train loss 1.1206, Train acc 0.5961\n","Eval loss 1.0597, Eval acc 0.6160\n","Saved checkpoint: ./checkpoints/vit_cifar10_best.pth\n","Training finished. Best val acc: 0.6160\n"]}],"source":["# ----------------------------\n","# Main\n","# ----------------------------\n","\n","batch_size = 128\n","epochs = 10\n","lr = 3e-4\n","weight_decay = 0.05\n","data_dir = './data'\n","output_dir = './checkpoints'\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","seed = 42\n","num_workers = 4\n","\n","\n","def set_seed(seed=42):\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed_all(seed)\n","\n","\n","set_seed(seed)\n","\n","device = torch.device(device if torch.cuda.is_available() and 'cuda' in device else 'cpu')\n","print(f\"Using device: {device}\")\n","\n","train_loader, test_loader = get_dataloaders(batch_size=batch_size, data_dir=data_dir, num_workers=num_workers)\n","\n","model = VisionTransformer(\n","    image_size=(3, 32, 32),\n","    patch_grid=(8, 8),\n","    embed_dim=128,\n","    ffn_dim=256,\n","    num_heads=4,\n","    num_blocks=6,\n","    num_classes=10,\n","    dropout=0.1,\n","    use_sinusoidal_pos=False,\n",")\n","model.to(device)\n","\n","# optimizer + scheduler\n","optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n","total_steps = epochs * len(train_loader)\n","scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=total_steps)\n","\n","best_acc = 0.0\n","for epoch in range(1, epochs + 1):\n","    train_loss, train_acc = train_one_epoch(model, device, train_loader, optimizer, epoch)\n","    val_loss, val_acc = evaluate(model, device, test_loader)\n","    # step scheduler once per epoch (approx)\n","    scheduler.step()\n","\n","    # checkpoint best\n","    if val_acc > best_acc:\n","        best_acc = val_acc\n","        save_checkpoint({\n","            'epoch': epoch,\n","            'model_state': model.state_dict(),\n","            'optimizer_state': optimizer.state_dict(),\n","            'val_acc': val_acc,\n","        }, output_dir, name=f\"vit_cifar10_best.pth\")\n","\n","print(f\"Training finished. Best val acc: {best_acc:.4f}\")"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9ba519bf","executionInfo":{"status":"ok","timestamp":1762408641638,"user_tz":-330,"elapsed":85,"user":{"displayName":"Ajeet Kumar","userId":"04620708898093466225"}},"outputId":"589c4adb-a97d-4109-8180-e296b6880c69"},"source":["checkpoint_path = os.path.join(output_dir, \"vit_cifar10_best.pth\")\n","checkpoint = torch.load(checkpoint_path, map_location=device)\n","\n","model_eval = VisionTransformer(\n","    image_size=(3, 32, 32),\n","    patch_grid=(8, 8),\n","    embed_dim=128,\n","    ffn_dim=256,\n","    num_heads=4,\n","    num_blocks=6,\n","    num_classes=10,\n","    dropout=0.1,\n","    use_sinusoidal_pos=False,\n",")\n","model_eval.load_state_dict(checkpoint['model_state'])\n","model_eval.to(device)"],"id":"9ba519bf","execution_count":11,"outputs":[{"output_type":"execute_result","data":{"text/plain":["VisionTransformer(\n","  (patch_embed): Linear(in_features=48, out_features=128, bias=True)\n","  (blocks): ModuleList(\n","    (0-5): 6 x TransformerEncoderBlock(\n","      (ln1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n","      (mha): MultiheadAttention(\n","        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n","      )\n","      (ln2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n","      (ffn): Sequential(\n","        (0): Linear(in_features=128, out_features=256, bias=True)\n","        (1): GELU(approximate='none')\n","        (2): Dropout(p=0.1, inplace=False)\n","        (3): Linear(in_features=256, out_features=128, bias=True)\n","        (4): Dropout(p=0.1, inplace=False)\n","      )\n","    )\n","  )\n","  (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n","  (head): Linear(in_features=128, out_features=10, bias=True)\n",")"]},"metadata":{},"execution_count":11}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"b8c96b23","executionInfo":{"status":"ok","timestamp":1762408659973,"user_tz":-330,"elapsed":1612,"user":{"displayName":"Ajeet Kumar","userId":"04620708898093466225"}},"outputId":"0ee43c99-f7fe-4223-9a7f-30f77217c7e0"},"source":["_, real_data_loader = get_dataloaders(batch_size=batch_size, data_dir=data_dir, num_workers=num_workers)"],"id":"b8c96b23","execution_count":12,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2fce48a3","executionInfo":{"status":"ok","timestamp":1762408674776,"user_tz":-330,"elapsed":24,"user":{"displayName":"Ajeet Kumar","userId":"04620708898093466225"}},"outputId":"d7fbe65a-c1ad-4e80-8c5f-ac9fc7d364d2"},"source":["print(get_dataloaders)"],"id":"2fce48a3","execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["<function get_dataloaders at 0x7dd949321d00>\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"69aa6980","executionInfo":{"status":"ok","timestamp":1762408692756,"user_tz":-330,"elapsed":2417,"user":{"displayName":"Ajeet Kumar","userId":"04620708898093466225"}},"outputId":"f1acdea5-8f5c-47fb-d506-f4330e2ff5a9"},"source":["model_eval.eval()\n","real_data_loss, real_data_acc = evaluate(model_eval, device, real_data_loader)\n","print(f\"Evaluation on real dataset: Loss {real_data_loss:.4f}, Accuracy {real_data_acc:.4f}\")"],"id":"69aa6980","execution_count":14,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["Eval loss 1.0597, Eval acc 0.6160\n","Evaluation on real dataset: Loss 1.0597, Accuracy 0.6160\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"60f01b13","executionInfo":{"status":"ok","timestamp":1762408714884,"user_tz":-330,"elapsed":521,"user":{"displayName":"Ajeet Kumar","userId":"04620708898093466225"}},"outputId":"4c5f6004-68ef-4543-8417-fc26cb1249b7"},"source":["model_eval.eval()\n","predictions = []\n","with torch.no_grad():\n","    for X, y in real_data_loader:\n","        X = X.to(device)\n","        logits = model_eval(X)\n","        preds = torch.argmax(logits, dim=1)\n","        predictions.extend(preds.cpu().tolist())\n","\n","print(f\"Number of predictions: {len(predictions)}\")"],"id":"60f01b13","execution_count":15,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["Number of predictions: 10000\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0904dfbd","executionInfo":{"status":"ok","timestamp":1762408735847,"user_tz":-330,"elapsed":3096,"user":{"displayName":"Ajeet Kumar","userId":"04620708898093466225"}},"outputId":"dd335723-d324-4157-c66e-3d8b2f01e93e"},"source":["from sklearn.metrics import classification_report, confusion_matrix\n","import numpy as np\n","\n","# Extract true labels from the DataLoader\n","true_labels = []\n","for _, y in real_data_loader:\n","    true_labels.extend(y.cpu().tolist())\n","\n","# Ensure the number of predictions and true labels match\n","print(f\"Number of true labels: {len(true_labels)}\")\n","print(f\"Number of predictions: {len(predictions)}\")\n","if len(true_labels) != len(predictions):\n","    print(\"Warning: Number of true labels and predictions do not match.\")\n","\n","\n","# Calculate and print classification report\n","print(\"\\nClassification Report:\")\n","print(classification_report(true_labels, predictions))\n","\n","# Calculate and print confusion matrix\n","print(\"\\nConfusion Matrix:\")\n","conf_matrix = confusion_matrix(true_labels, predictions)\n","print(conf_matrix)\n","\n","# Interpret the results\n","print(\"\\nInterpretation:\")\n","print(f\"Overall Accuracy: {real_data_acc:.4f}\")\n","print(\"The classification report provides precision, recall, and F1-score for each class.\")\n","print(\"Precision indicates the accuracy of positive predictions for each class.\")\n","print(\"Recall indicates the ability of the model to find all positive samples for each class.\")\n","print(\"The F1-score is the harmonic mean of precision and recall.\")\n","print(\"The confusion matrix shows the number of correct and incorrect predictions for each class.\")\n","print(\"Diagonal elements represent correct predictions, while off-diagonal elements represent misclassifications.\")"],"id":"0904dfbd","execution_count":16,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["Number of true labels: 10000\n","Number of predictions: 10000\n","\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.57      0.73      0.64      1000\n","           1       0.63      0.78      0.70      1000\n","           2       0.52      0.52      0.52      1000\n","           3       0.51      0.31      0.39      1000\n","           4       0.59      0.55      0.57      1000\n","           5       0.53      0.56      0.54      1000\n","           6       0.67      0.74      0.70      1000\n","           7       0.69      0.68      0.68      1000\n","           8       0.73      0.74      0.74      1000\n","           9       0.69      0.55      0.62      1000\n","\n","    accuracy                           0.62     10000\n","   macro avg       0.61      0.62      0.61     10000\n","weighted avg       0.61      0.62      0.61     10000\n","\n","\n","Confusion Matrix:\n","[[730  40  30  14  17   2  25   7 103  32]\n"," [ 66 778  14   3   5   1   6   5  32  90]\n"," [106  16 519  47 109  68  71  32  18  14]\n"," [ 37  18  98 311  61 272 120  41  21  21]\n"," [ 44   7 125  35 548  48  59 115  13   6]\n"," [ 37   9  84 116  52 557  39  74  14  18]\n"," [ 12  14  62  43  69  22 742  19   7  10]\n"," [ 54  11  44  24  54  71  18 679  13  32]\n"," [117  68   8  13  15   4   9   2 743  21]\n"," [ 73 267  10   9   3   3  17  12  53 553]]\n","\n","Interpretation:\n","Overall Accuracy: 0.6160\n","The classification report provides precision, recall, and F1-score for each class.\n","Precision indicates the accuracy of positive predictions for each class.\n","Recall indicates the ability of the model to find all positive samples for each class.\n","The F1-score is the harmonic mean of precision and recall.\n","The confusion matrix shows the number of correct and incorrect predictions for each class.\n","Diagonal elements represent correct predictions, while off-diagonal elements represent misclassifications.\n"]}]},{"cell_type":"markdown","metadata":{"id":"4e5b4421"},"source":["## Summary:\n","\n","### Data Analysis Key Findings\n","\n","*   The trained Vision Transformer model achieved an accuracy of 61.60% and a loss of 1.0597 on the real CIFAR-10 test dataset.\n","*   The analysis of predictions revealed varying performance across different classes in the CIFAR-10 dataset, as shown by the precision, recall, and F1-scores in the classification report.\n","*   The confusion matrix indicated the distribution of correct and incorrect predictions for each class, highlighting which classes the model struggled to classify correctly.\n","\n","### Insights or Next Steps\n","\n","*   Investigate the specific classes where the model shows lower performance (lower precision, recall, and F1-score) to understand potential causes, such as class imbalance or visual similarities between misclassified classes.\n","*   Consider techniques like data augmentation, transfer learning with a pre-trained model on a larger dataset, or fine-tuning the model architecture or hyperparameters to improve performance, especially for the underperforming classes.\n"],"id":"4e5b4421"}],"metadata":{"language_info":{"name":"python"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU","kernelspec":{"name":"python3","display_name":"Python 3"}},"nbformat":4,"nbformat_minor":5}